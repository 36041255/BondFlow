{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_all_gradients(model, epoch=None):\n",
    "    \"\"\"打印模型中所有可训练参数的梯度信息\"\"\"\n",
    "    if epoch is not None:\n",
    "        print(f\"\\n=== Epoch {epoch} 梯度信息 ===\")\n",
    "    else:\n",
    "        print(f\"\\n=== 当前梯度信息 ===\")\n",
    "    \n",
    "    # 收集所有梯度信息\n",
    "    grad_data = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad = param.grad.data\n",
    "            grad_info = {\n",
    "                \"name\": name,\n",
    "                \"shape\": tuple(grad.shape),\n",
    "                \"dtype\": str(grad.dtype),\n",
    "                \"norm\": grad.norm().item(),\n",
    "                \"min\": grad.min().item(),\n",
    "                \"max\": grad.max().item(),\n",
    "                \"mean\": grad.mean().item(),\n",
    "                \"nan\": torch.isnan(grad).sum().item(),\n",
    "                \"inf\": torch.isinf(grad).sum().item(),\n",
    "                \"zero\": (grad == 0).sum().item()\n",
    "            }\n",
    "            grad_data.append(grad_info)\n",
    "    \n",
    "    # 按梯度范数排序（从大到小）\n",
    "    grad_data.sort(key=lambda x: x[\"norm\"], reverse=True)\n",
    "    \n",
    "    # 打印表格头\n",
    "    print(f\"{'参数名称':<40} | {'形状':<20} | {'范数':>10} | {'NaN':>5} | {'Inf':>5} | {'零值%':>6} | {'范围'}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # 打印每个参数的梯度信息\n",
    "    for info in grad_data:\n",
    "        zero_percent = info[\"zero\"] / torch.numel(torch.zeros(info['shape'])) * 100\n",
    "        range_str = f\"[{info['min']:.3e}, {info['max']:.3e}]\"\n",
    "        \n",
    "        # 高亮异常梯度\n",
    "        if info[\"nan\"] > 0 or info[\"inf\"] > 0:\n",
    "            highlight = \"\\033[91m\"  # 红色\n",
    "            reset = \"\\033[0m\"\n",
    "        else:\n",
    "            highlight = reset = \"\"\n",
    "        \n",
    "        print(f\"{highlight}{info['name']:<40} | {str(info['shape']):<20} | \"\n",
    "            f\"{info['norm']:>10.3e} | \"\n",
    "            f\"{info['nan']:>5} | \"\n",
    "            f\"{info['inf']:>5} | \"\n",
    "            f\"{zero_percent:>5.1f}% | \"\n",
    "            f\"{range_str}{reset}\")\n",
    "    \n",
    "    # 打印统计摘要\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    grad_params = sum(info['shape'][0] for info in grad_data)\n",
    "    nan_params = sum(info['nan'] for info in grad_data)\n",
    "    inf_params = sum(info['inf'] for info in grad_data)\n",
    "    \n",
    "    print(\"\\n摘要:\")\n",
    "    print(f\"• 总可训练参数: {total_params:,}\")\n",
    "    print(f\"• 有梯度的参数: {grad_params} ({grad_params/total_params:.1%})\")\n",
    "    print(f\"• NaN 梯度值总数: {nan_params}\")\n",
    "    print(f\"• Inf 梯度值总数: {inf_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713de032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "PyTorch: 2.6.0+cu124\n",
      "scaled_dot_product_attention available: True\n",
      "FP32 forward time: 0.391782 seconds\n",
      "fp32: shape torch.Size([2, 5, 200]) dtype torch.float32 has_nan False\n",
      "AMP forward time: 0.149990 seconds\n",
      "amp:  shape torch.Size([2, 5, 200]) dtype torch.float16 has_nan False\n",
      "SDPA forward time: 0.000487 seconds\n",
      "sdpa: shape torch.Size([2, 5, 200]) dtype torch.float16 has_nan False\n",
      "max_abs_diff fp32 vs amp = 2.413094e-04\n",
      "max_abs_diff fp32 vs sdpa = 2.335757e-04\n",
      "max_abs_diff amp vs sdpa = 2.441406e-04\n",
      "\n",
      "--- sample values for fp32 (slice [0, :3, :6]) dtype=torch.float32 ---\n",
      "tensor([[ 0.0273,  0.0148, -0.0659, -0.0573,  0.2050,  0.0022],\n",
      "        [ 0.0096,  0.0128, -0.0640, -0.0453,  0.2135,  0.0026],\n",
      "        [ 0.0100,  0.0167, -0.0386, -0.0476,  0.1880, -0.0046]])\n",
      "\n",
      "--- sample values for amp (slice [0, :3, :6]) dtype=torch.float16 ---\n",
      "tensor([[ 0.0272,  0.0148, -0.0658, -0.0573,  0.2050,  0.0022],\n",
      "        [ 0.0096,  0.0129, -0.0641, -0.0453,  0.2136,  0.0026],\n",
      "        [ 0.0099,  0.0167, -0.0385, -0.0475,  0.1880, -0.0045]],\n",
      "       dtype=torch.float16)\n",
      "\n",
      "--- sample values for sdpa (slice [0, :3, :6]) dtype=torch.float16 ---\n",
      "tensor([[ 0.0273,  0.0148, -0.0658, -0.0573,  0.2050,  0.0022],\n",
      "        [ 0.0095,  0.0129, -0.0640, -0.0453,  0.2136,  0.0027],\n",
      "        [ 0.0100,  0.0166, -0.0385, -0.0476,  0.1880, -0.0045]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2923799/4066549279.py:202: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast_ctx():\n",
      "/tmp/ipykernel_2923799/4066549279.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast_ctx():\n"
     ]
    }
   ],
   "source": [
    "# test_attention_sdpa.py\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "_HAS_SDPA = hasattr(F, \"scaled_dot_product_attention\")\n",
    "print(\"scaled_dot_product_attention available:\", _HAS_SDPA)\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def _mask_to_bool(mask):\n",
    "    \"\"\"Convert mask (None or 0/1 int tensor or bool) -> boolean mask where True = invalid (padding).\"\"\"\n",
    "    if mask is None:\n",
    "        return None\n",
    "    return (mask == 0) if mask.dtype != torch.bool else mask\n",
    "\n",
    "def _stable_softmax_on_last_dim(logits, dim, attn_mask_bool=None):\n",
    "    \"\"\"Perform softmax in float32 for stability, then cast back to logits.dtype.\"\"\"\n",
    "    logits_f32 = logits.float()\n",
    "    if attn_mask_bool is not None:\n",
    "        logits_f32 = logits_f32.masked_fill(attn_mask_bool, -1e9)\n",
    "    probs_f32 = F.softmax(logits_f32, dim=dim)\n",
    "    return probs_f32.to(logits.dtype)\n",
    "\n",
    "def _sdpa_forward(q, k, v, mask_bool=None, bias_add=None, dropout_p=0.0, is_causal=False):\n",
    "    \"\"\"\n",
    "    q: (B, Q, H, D)\n",
    "    k: (B, K, H, D)\n",
    "    v: (B, K, H, D)\n",
    "    mask_bool: either None, (B, L) or (B, Q, K) boolean mask with True=invalid\n",
    "    bias_add: optional additive bias to logits (broadcastable)\n",
    "    returns: (B, Q, H, D)\n",
    "    \"\"\"\n",
    "    q_sd = q.permute(0, 2, 1, 3).contiguous()  # (B, H, Q, D)\n",
    "    k_sd = k.permute(0, 2, 1, 3).contiguous()  # (B, H, K, D)\n",
    "    v_sd = v.permute(0, 2, 1, 3).contiguous()  # (B, H, K, D)\n",
    "    B, H, Q, D = q_sd.shape\n",
    "    _, _, K, _ = k_sd.shape\n",
    "\n",
    "    attn_mask = None\n",
    "    if mask_bool is not None:\n",
    "        if mask_bool.dim() == 2:\n",
    "            # mask_bool: (B, L) -> build (B, Q, K)\n",
    "            mask_q = mask_bool[:, :Q]\n",
    "            mask_k = mask_bool[:, :K]\n",
    "            mask_2d = mask_q.unsqueeze(2) | mask_k.unsqueeze(1)  # (B, Q, K)\n",
    "            mask_float = mask_2d.to(q_sd.dtype) * (-1e9)\n",
    "        elif mask_bool.dim() == 3:\n",
    "            # already (B, Q, K)\n",
    "            mask_float = mask_bool.to(q_sd.dtype) * (-1e9)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported mask_bool dim in SDPA wrapper\")\n",
    "        attn_mask = mask_float.unsqueeze(1)  # (B,1,Q,K) to broadcast to heads\n",
    "\n",
    "    if bias_add is not None:\n",
    "        b = bias_add\n",
    "        # attempt common rearrangements to (B, H, Q, K)\n",
    "        if b.dim() == 4 and b.shape[-1] == H:\n",
    "            b = b.permute(0, 3, 1, 2)  # (B,H,Q,K)\n",
    "        elif b.dim() == 3 and b.shape[-1] == H and b.shape[1] == K:\n",
    "            b = b.permute(0, 2, 1).unsqueeze(2)  # (B,H,1,K)\n",
    "        b = b.to(q_sd.dtype)\n",
    "        if attn_mask is None:\n",
    "            attn_mask = b\n",
    "        else:\n",
    "            attn_mask = attn_mask + b\n",
    "\n",
    "    # call PyTorch SDPA\n",
    "    out_sd = F.scaled_dot_product_attention(q_sd, k_sd, v_sd, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=is_causal)\n",
    "    out = out_sd.permute(0, 2, 1, 3).contiguous()  # (B, Q, H, D)\n",
    "    return out\n",
    "\n",
    "# -------------------- Attention class (use_sdpa optional) --------------------\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_query, d_key, n_head, d_hidden, d_out, use_sdpa=False):\n",
    "        super().__init__()\n",
    "        self.h = n_head\n",
    "        self.dim = d_hidden\n",
    "        self.to_q = nn.Linear(d_query, n_head * d_hidden, bias=False)\n",
    "        self.to_k = nn.Linear(d_key, n_head * d_hidden, bias=False)\n",
    "        self.to_v = nn.Linear(d_key, n_head * d_hidden, bias=False)\n",
    "        self.to_out = nn.Linear(n_head * d_hidden, d_out)\n",
    "        self.scaling = 1.0 / math.sqrt(d_hidden)\n",
    "        self.use_sdpa = use_sdpa and _HAS_SDPA\n",
    "\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        mask: None or (B, L) with 1=valid, 0=invalid (padding)\n",
    "        output: (B, Q, d_out)\n",
    "        \"\"\"\n",
    "        mask_bool = _mask_to_bool(mask)  # True = invalid\n",
    "        B, Q = query.shape[:2]\n",
    "        _, K = key.shape[:2]\n",
    "\n",
    "        # project\n",
    "        q_full = self.to_q(query).reshape(B, Q, self.h, self.dim)\n",
    "        k = self.to_k(key).reshape(B, K, self.h, self.dim)\n",
    "        v = self.to_v(value).reshape(B, K, self.h, self.dim)\n",
    "\n",
    "        if self.use_sdpa:\n",
    "            # DO NOT apply manual scaling to q before SDPA (SDPA handles scaling)\n",
    "            # build mask_2d if mask_bool is (B,L)\n",
    "            if mask_bool is not None and mask_bool.dim() == 2:\n",
    "                mask_2d = mask_bool[:, :Q].unsqueeze(2) | mask_bool[:, :K].unsqueeze(1)  # (B,Q,K)\n",
    "            else:\n",
    "                mask_2d = mask_bool\n",
    "            out = _sdpa_forward(q_full, k, v, mask_bool=mask_2d, bias_add=None)\n",
    "            out = out.reshape(B, Q, self.h * self.dim)\n",
    "            out = self.to_out(out)\n",
    "            if mask_bool is not None:\n",
    "                out = out.masked_fill(mask_bool[:, :Q].unsqueeze(-1), 0.0)\n",
    "            return out\n",
    "\n",
    "        # fallback: scale q manually and use stable softmax\n",
    "        q = q_full * self.scaling\n",
    "        attn_logits = einsum('bqhd,bkhd->bhqk', q, k)  # (B, H, Q, K)\n",
    "\n",
    "        if mask_bool is not None:\n",
    "            mask_q = mask_bool[:, :Q]\n",
    "            mask_k = mask_bool[:, :K]\n",
    "            mask_2d = mask_q.unsqueeze(2) | mask_k.unsqueeze(1)  # (B, Q, K)\n",
    "            attn_for_mask = mask_2d.unsqueeze(1)  # (B,1,Q,K)\n",
    "        else:\n",
    "            attn_for_mask = None\n",
    "\n",
    "        attn = _stable_softmax_on_last_dim(attn_logits, dim=-1, attn_mask_bool=attn_for_mask)\n",
    "        out = einsum('bhqk,bkhd->bqhd', attn, v)\n",
    "        out = out.reshape(B, Q, self.h * self.dim)\n",
    "        out = self.to_out(out)\n",
    "        if mask_bool is not None:\n",
    "            out = out.masked_fill(mask_bool[:, :Q].unsqueeze(-1), 0.0)\n",
    "        return out\n",
    "\n",
    "# -------------------- Test routine --------------------\n",
    "import time\n",
    "def run_test():\n",
    "    # small random test settings\n",
    "    B = 2\n",
    "    Q = 5\n",
    "    K = 6\n",
    "    d_query = 160\n",
    "    d_key = 160\n",
    "    n_head = 40\n",
    "    d_hidden = 80\n",
    "    d_out = 200\n",
    "\n",
    "    # generate random inputs\n",
    "    query = torch.randn(B, Q, d_query, device=device)\n",
    "    key   = torch.randn(B, K, d_key, device=device)\n",
    "    value = torch.randn(B, K, d_key, device=device)\n",
    "    mask = torch.ones(B, max(Q, K), dtype=torch.int8, device=device)\n",
    "    mask[0, 3:] = 0\n",
    "    mask[1, 4:] = 0\n",
    "\n",
    "    # create model and save state\n",
    "    model = Attention(d_query, d_key, n_head, d_hidden, d_out, use_sdpa=False).to(device)\n",
    "    state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    def load_state(m, s):\n",
    "        mdl_state = {k: v.clone() for k, v in s.items()}\n",
    "        device_ = next(m.parameters()).device if any(True for _ in m.parameters()) else device\n",
    "        for k in mdl_state:\n",
    "            mdl_state[k] = mdl_state[k].to(device_)\n",
    "        m.load_state_dict(mdl_state)\n",
    "\n",
    "    results = {}\n",
    "    names = []\n",
    "\n",
    "    # determine autocast context for current device\n",
    "    if device.type == \"cuda\":\n",
    "        autocast_ctx = torch.cuda.amp.autocast\n",
    "    else:\n",
    "        # prefer torch.cpu.amp.autocast if available, else nullcontext\n",
    "        autocast_ctx = getattr(torch.cpu.amp, \"autocast\", nullcontext)\n",
    "\n",
    "    # 1) FP32 baseline (no autocast), use_sdpa=False\n",
    "    load_state(model, state)\n",
    "    model.use_sdpa = False\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        out_fp32 = model(query, key, value, mask=mask)\n",
    "        print(f\"FP32 forward time: {time.time() - start_time:.6f} seconds\")\n",
    "    results['fp32'] = out_fp32.detach(); names.append('fp32')\n",
    "    print(\"fp32: shape\", out_fp32.shape, \"dtype\", out_fp32.dtype, \"has_nan\", torch.isnan(out_fp32).any().item())\n",
    "\n",
    "    # 2) AMP (autocast) with fallback softmax (use_sdpa=False)\n",
    "    load_state(model, state)\n",
    "    model.use_sdpa = False\n",
    "    model.eval()\n",
    "    with torch.no_grad(), autocast_ctx():\n",
    "        start_time = time.time()\n",
    "        out_amp = model(query, key, value, mask=mask)\n",
    "        print(f\"AMP forward time: {time.time() - start_time:.6f} seconds\")  \n",
    "    results['amp'] = out_amp.detach(); names.append('amp')\n",
    "    print(\"amp:  shape\", out_amp.shape, \"dtype\", out_amp.dtype, \"has_nan\", torch.isnan(out_amp).any().item())\n",
    "\n",
    "    # 3) SDPA path (if available) with autocast\n",
    "    if _HAS_SDPA:\n",
    "        load_state(model, state)\n",
    "        model.use_sdpa = True\n",
    "        model.eval()\n",
    "        with torch.no_grad(), autocast_ctx():\n",
    "            start_time = time.time()\n",
    "            out_sdpa = model(query, key, value, mask=mask)\n",
    "            print(f\"SDPA forward time: {time.time() - start_time:.6f} seconds\")\n",
    "        results['sdpa'] = out_sdpa.detach(); names.append('sdpa')\n",
    "        print(\"sdpa: shape\", out_sdpa.shape, \"dtype\", out_sdpa.dtype, \"has_nan\", torch.isnan(out_sdpa).any().item())\n",
    "    else:\n",
    "        print(\"SDPA unavailable; skipping sdpa run\")\n",
    "\n",
    "    # compute pairwise max absolute differences\n",
    "    def max_abs_diff(a, b):\n",
    "        return (a.to(torch.float32) - b.to(torch.float32)).abs().max().item()\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        for j in range(i + 1, len(names)):\n",
    "            print(f\"max_abs_diff {names[i]} vs {names[j]} = {max_abs_diff(results[names[i]], results[names[j]]):.6e}\")\n",
    "\n",
    "    # print small slice of outputs for inspection\n",
    "    for k in names:\n",
    "        t = results[k].cpu()\n",
    "        print(f\"\\n--- sample values for {k} (slice [0, :3, :6]) dtype={t.dtype} ---\")\n",
    "        print(t[0, :3, :6])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e488eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在使用设备: cuda\n",
      "设备名称: NVIDIA A800-SXM4-80GB\n",
      "--------------------------------------------------\n",
      "测试参数 (较大负载):\n",
      "  Batch Size: 4, MSA Depth: 64, Seq Len: 256\n",
      "  D_MSA: 64, D_PAIR: 32\n",
      "  Heads: 4, Hidden Dim: 16\n",
      "--------------------------------------------------\n",
      "--- 正在检查 Attention 的一致性 ---\n",
      "❌ FAIL: 输出不一致。最大差异: 7.560725e-01\n",
      "-----------------------------\n",
      "--- 正在进行 Attention 的性能基准测试 ---\n",
      "  - 原始版本...\n",
      "  - 优化版本...\n",
      "--- 正在检查 AttentionWithBias 的一致性 ---\n",
      "✅ PASS: 输出一致。最大差异: 4.768372e-07\n",
      "-------------------------------------\n",
      "--- 正在进行 AttentionWithBias 的性能基准测试 ---\n",
      "  - 原始版本...\n",
      "  - 优化版本...\n",
      "--- 正在检查 MSARowAttention 的一致性 ---\n",
      "✅ PASS: 输出一致。最大差异: 0.000000e+00\n",
      "-----------------------------------\n",
      "--- 正在进行 MSARowAttention 的性能基准测试 ---\n",
      "  - 原始版本...\n",
      "  - 优化版本...\n",
      "--- 正在检查 BiasedAxialAttention 的一致性 ---\n",
      "✅ PASS: 输出一致。最大差异: 0.000000e+00\n",
      "----------------------------------------\n",
      "--- 正在进行 BiasedAxialAttention 的性能基准测试 ---\n",
      "  - 原始版本...\n",
      "  - 优化版本...\n",
      "\n",
      "================================================================================\n",
      "基准测试最终结果\n",
      "================================================================================\n",
      "| 模块名                    | 版本       | 时间 (ms/iter)     | 内存 (MB)      | 提升         |\n",
      "|------------------------|----------|------------------|--------------|------------|\n",
      "| Attention              | 原始       | 0.882            | 38.40        | -          |\n",
      "|                        | 优化       | 0.557            | 21.43        |  36.85% (T) |\n",
      "|                        |          |                  |              |  44.19% (M) |\n",
      "|------------------------|----------|------------------|--------------|------------|\n",
      "| AttentionWithBias      | 原始       | 3.849            | 53.07        | -          |\n",
      "|                        | 优化       | 2.608            | 43.12        |  32.23% (T) |\n",
      "|                        |          |                  |              |  18.75% (M) |\n",
      "|------------------------|----------|------------------|--------------|------------|\n",
      "| MSARowAttention        | 原始       | 8.786            | 367.46       | -          |\n",
      "|                        | 优化       | 6.321            | 415.46       |  28.05% (T) |\n",
      "|                        |          |                  |              | -13.06% (M) |\n",
      "|------------------------|----------|------------------|--------------|------------|\n",
      "| BiasedAxialAttention   | 原始       | 19.386           | 813.04       | -          |\n",
      "|                        | 优化       | 15.797           | 941.04       |  18.51% (T) |\n",
      "|                        |          |                  |              | -15.74% (M) |\n",
      "|------------------------|----------|------------------|--------------|------------|\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from einops import rearrange\n",
    "from opt_einsum import contract as einsum\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 辅助函数 (模拟外部依赖)\n",
    "# =============================================================================\n",
    "def init_lecun_normal(module):\n",
    "    \"\"\"模拟 rfdiffusion.util_module.init_lecun_normal\"\"\"\n",
    "    nn.init.xavier_normal_(module.weight)\n",
    "    return module\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 定义模型 - Attention\n",
    "# =============================================================================\n",
    "\n",
    "class AttentionOriginal(nn.Module):\n",
    "    def __init__(self, d_query, d_key, n_head, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.h, self.dim = n_head, d_hidden\n",
    "        self.to_q = nn.Linear(d_query, n_head*d_hidden, bias=False)\n",
    "        self.to_k = nn.Linear(d_key, n_head*d_hidden, bias=False)\n",
    "        self.to_v = nn.Linear(d_key, n_head*d_hidden, bias=False)\n",
    "        self.to_out = nn.Linear(n_head*d_hidden, d_out)\n",
    "        self.scaling = 1/math.sqrt(d_hidden)\n",
    "        self.reset_parameter()\n",
    "    def reset_parameter(self):\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        bool_mask = (mask == 0)\n",
    "        B, Q, _, K = *query.shape[:2], *key.shape[:2]\n",
    "        q = self.to_q(query).reshape(B, Q, self.h, self.dim)\n",
    "        k = self.to_k(key).reshape(B, K, self.h, self.dim)\n",
    "        v = self.to_v(value).reshape(B, K, self.h, self.dim)\n",
    "        q = q * self.scaling\n",
    "        attn = einsum('bqhd,bkhd->bhqk', q, k)\n",
    "        mask_2d = bool_mask.unsqueeze(1) | bool_mask.unsqueeze(2)\n",
    "        attn = attn.masked_fill(mask_2d.unsqueeze(1), -1e9)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = einsum('bhqk,bkhd->bqhd', attn, v)\n",
    "        out = out.reshape(B, Q, self.h*self.dim)\n",
    "        out = self.to_out(out)\n",
    "        if mask is not None: out = out.masked_fill(bool_mask.unsqueeze(-1), 0.0)\n",
    "        return out\n",
    "\n",
    "class AttentionOptimized(nn.Module):\n",
    "    def __init__(self, d_query, d_key, n_head, d_hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.h, self.d_hidden = n_head, d_hidden\n",
    "        self.to_q = nn.Linear(d_query, n_head * d_hidden, bias=False)\n",
    "        self.to_kv = nn.Linear(d_key, 2 * n_head * d_hidden, bias=False)\n",
    "        self.to_out = nn.Linear(n_head * d_hidden, d_out)\n",
    "        self.reset_parameter()\n",
    "    def reset_parameter(self):\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_kv.weight)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        q = self.to_q(query)\n",
    "        k, v = self.to_kv(key).chunk(2, dim=-1)\n",
    "        \n",
    "        q = rearrange(q, 'b q (h d) -> b h q d', h=self.h)\n",
    "        k = rearrange(k, 'b k (h d) -> b h k d', h=self.h)\n",
    "        v = rearrange(v, 'b k (h d) -> b h k d', h=self.h)\n",
    "        \n",
    "        attn_mask = None\n",
    "        if mask is not None:\n",
    "            bool_mask = (mask == 0)\n",
    "            mask_2d = bool_mask.unsqueeze(1) | bool_mask.unsqueeze(2)\n",
    "            attn_mask = mask_2d.unsqueeze(1)\n",
    "            \n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, is_causal=False)\n",
    "        out = rearrange(out, 'b h q d -> b q (h d)')\n",
    "        out = self.to_out(out)\n",
    "        if mask is not None: out = out.masked_fill((mask == 0).unsqueeze(-1), 0.0)\n",
    "        return out\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 定义模型 - AttentionWithBias\n",
    "# =============================================================================\n",
    "\n",
    "class AttentionWithBiasOriginal(nn.Module):\n",
    "    def __init__(self, d_in=256, d_bias=128, n_head=8, d_hidden=32):\n",
    "        super().__init__()\n",
    "        self.norm_in, self.norm_bias = nn.LayerNorm(d_in), nn.LayerNorm(d_bias)\n",
    "        self.to_q, self.to_k, self.to_v = nn.Linear(d_in, n_head*d_hidden, bias=False), nn.Linear(d_in, n_head*d_hidden, bias=False), nn.Linear(d_in, n_head*d_hidden, bias=False)\n",
    "        self.to_b, self.to_g, self.to_out = nn.Linear(d_bias, n_head, bias=False), nn.Linear(d_in, n_head*d_hidden), nn.Linear(n_head*d_hidden, d_in)\n",
    "        self.scaling, self.h, self.dim = 1 / math.sqrt(d_hidden), n_head, d_hidden\n",
    "        self.reset_parameter()\n",
    "    def reset_parameter(self):\n",
    "        for w in [self.to_q.weight, self.to_k.weight, self.to_v.weight]: nn.init.xavier_uniform_(w)\n",
    "        self.to_b = init_lecun_normal(self.to_b)\n",
    "        if hasattr(self.to_g, 'bias') and self.to_g.bias is not None:\n",
    "            nn.init.ones_(self.to_g.bias)\n",
    "    def forward(self, x, bias, mask=None):\n",
    "        B,L = x.shape[:2]\n",
    "        bool_mask = (mask == 0)\n",
    "        x_norm, bias_norm = self.norm_in(torch.nan_to_num(x)), self.norm_bias(torch.nan_to_num(bias))\n",
    "        query, key, value = self.to_q(x_norm).reshape(B, L, self.h, self.dim), self.to_k(x_norm).reshape(B, L, self.h, self.dim), self.to_v(x_norm).reshape(B, L, self.h, self.dim)\n",
    "        bias_h, gate = self.to_b(bias_norm), torch.sigmoid(self.to_g(x_norm))\n",
    "        key = key * self.scaling\n",
    "        attn = einsum('bqhd,bkhd->bhqk', query, key)\n",
    "        attn = rearrange(attn, 'b h q k -> b q k h') + bias_h\n",
    "        if bool_mask.any():\n",
    "            mask_2d = bool_mask.unsqueeze(1) | bool_mask.unsqueeze(2)\n",
    "            attn.masked_fill_(mask_2d.unsqueeze(-1), -1e9)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        attn = rearrange(attn, 'b q k h -> b h q k')\n",
    "        out = einsum('bhqk,bkhd->bqhd', attn, value).reshape(B, L, -1)\n",
    "        out = gate * out\n",
    "        out = self.to_out(out)\n",
    "        if bool_mask.any(): out = out.masked_fill(bool_mask.unsqueeze(-1), 0.0)\n",
    "        return out\n",
    "\n",
    "class AttentionWithBiasOptimized(nn.Module):\n",
    "    def __init__(self, d_in=256, d_bias=128, n_head=8, d_hidden=32):\n",
    "        super().__init__()\n",
    "        self.norm_in, self.norm_bias = nn.LayerNorm(d_in), nn.LayerNorm(d_bias)\n",
    "        self.to_qkv = nn.Linear(d_in, 3 * n_head * d_hidden, bias=False)\n",
    "        self.to_b, self.to_g, self.to_out = nn.Linear(d_bias, n_head, bias=False), nn.Linear(d_in, n_head*d_hidden), nn.Linear(n_head*d_hidden, d_in)\n",
    "        self.h = n_head\n",
    "        self.reset_parameter()\n",
    "    def reset_parameter(self):\n",
    "        nn.init.xavier_uniform_(self.to_qkv.weight)\n",
    "        self.to_b = init_lecun_normal(self.to_b)\n",
    "        if hasattr(self.to_g, 'bias') and self.to_g.bias is not None:\n",
    "            nn.init.ones_(self.to_g.bias)\n",
    "    def forward(self, x, bias, mask=None):\n",
    "        x_norm, bias_norm = self.norm_in(torch.nan_to_num(x)), self.norm_bias(torch.nan_to_num(bias))\n",
    "        q, k, v = self.to_qkv(x_norm).chunk(3, dim=-1)\n",
    "        q, k, v = [rearrange(t, 'b l (h d) -> b h l d', h=self.h) for t in (q, k, v)]\n",
    "        \n",
    "        float_mask = self.to_b(bias_norm)\n",
    "        float_mask = rearrange(float_mask, 'b q k h -> b h q k')\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = (mask == 0)\n",
    "            mask_2d = padding_mask.unsqueeze(1) | padding_mask.unsqueeze(2)\n",
    "            float_mask = float_mask.masked_fill(mask_2d.unsqueeze(1), -torch.inf)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=float_mask)\n",
    "        out = rearrange(out, 'b h l d -> b l (h d)')\n",
    "        \n",
    "        gate = torch.sigmoid(self.to_g(x_norm))\n",
    "        out = gate * out\n",
    "        out = self.to_out(out)\n",
    "        if mask is not None: out = out.masked_fill((mask == 0).unsqueeze(-1), 0.0)\n",
    "        return out\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 定义模型 - MSARowAttentionWithBias\n",
    "# =============================================================================\n",
    "\n",
    "class MSARowAttentionWithBiasOriginal(nn.Module):\n",
    "    def __init__(self, d_msa=256, d_pair=128, n_head=8, d_hidden=32):\n",
    "        super().__init__()\n",
    "        self.norm_msa, self.norm_pair = nn.LayerNorm(d_msa), nn.LayerNorm(d_pair)\n",
    "        self.to_q, self.to_k, self.to_v = nn.Linear(d_msa, n_head*d_hidden, bias=False), nn.Linear(d_msa, n_head*d_hidden, bias=False), nn.Linear(d_msa, n_head*d_hidden, bias=False)\n",
    "        self.to_b, self.to_g, self.to_out = nn.Linear(d_pair, n_head, bias=False), nn.Linear(d_msa, n_head*d_hidden), nn.Linear(n_head*d_hidden, d_msa)\n",
    "        self.scaling, self.h, self.dim = 1/math.sqrt(d_hidden), n_head, d_hidden\n",
    "        self.reset_parameter()\n",
    "    def reset_parameter(self):\n",
    "        for w in [self.to_q.weight, self.to_k.weight, self.to_v.weight]: nn.init.xavier_uniform_(w)\n",
    "        self.to_b = init_lecun_normal(self.to_b)\n",
    "        if hasattr(self.to_g, 'bias') and self.to_g.bias is not None:\n",
    "            nn.init.ones_(self.to_g.bias)\n",
    "    def forward(self, msa, pair, mask=None):\n",
    "        B, N, L, _ = msa.shape\n",
    "        bool_mask = (mask == 0)\n",
    "        msa_norm, pair_norm = self.norm_msa(torch.nan_to_num(msa)), self.norm_pair(torch.nan_to_num(pair))\n",
    "        query, key, value = [t.reshape(B,N,L,self.h,self.dim) for t in (self.to_q(msa_norm), self.to_k(msa_norm), self.to_v(msa_norm))]\n",
    "        bias, gate = self.to_b(pair_norm), torch.sigmoid(self.to_g(msa_norm))\n",
    "        m_view = bool_mask.view(B, 1, L, 1, 1)\n",
    "        query, value, key = query.masked_fill(m_view, 0.0), value.masked_fill(m_view, 0.0), key.masked_fill(m_view, 0.0)\n",
    "        attn = einsum('bnqhd,bnkhd->bqkh', query*self.scaling, key) + bias\n",
    "        mask_2d = bool_mask.unsqueeze(1) | bool_mask.unsqueeze(2)\n",
    "        if mask_2d.any(): attn = attn.masked_fill(mask_2d.view(B, L, L, 1), -1e9)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        out = einsum('bqkh,bnkhd->bnqhd', attn, value).reshape(B, N, L, -1)\n",
    "        out = gate * out\n",
    "        out = self.to_out(out)\n",
    "        if bool_mask.any(): out = out.masked_fill(bool_mask.view(B, 1, L, 1), 0.0)\n",
    "        return out\n",
    "\n",
    "class MSARowAttentionWithBiasOptimized(nn.Module):\n",
    "    def __init__(self, d_msa=256, d_pair=128, n_head=8, d_hidden=32):\n",
    "        super().__init__()\n",
    "        self.norm_msa, self.norm_pair = nn.LayerNorm(d_msa), nn.LayerNorm(d_pair)\n",
    "        self.to_qkv = nn.Linear(d_msa, 3 * n_head * d_hidden, bias=False)\n",
    "        self.to_b, self.to_g, self.to_out = nn.Linear(d_pair, n_head, bias=False), nn.Linear(d_msa, n_head*d_hidden), nn.Linear(n_head*d_hidden, d_msa)\n",
    "        self.scaling, self.h, self.dim = 1/math.sqrt(d_hidden), n_head, d_hidden\n",
    "        self.reset_parameter()\n",
    "    def reset_parameter(self):\n",
    "        nn.init.xavier_uniform_(self.to_qkv.weight)\n",
    "        self.to_b = init_lecun_normal(self.to_b)\n",
    "        if hasattr(self.to_g, 'bias') and self.to_g.bias is not None:\n",
    "            nn.init.ones_(self.to_g.bias)\n",
    "    def forward(self, msa, pair, mask=None):\n",
    "        B, N, L, _ = msa.shape\n",
    "        bool_mask = (mask == 0)\n",
    "        msa_norm, pair_norm = self.norm_msa(torch.nan_to_num(msa)), self.norm_pair(torch.nan_to_num(pair))\n",
    "        \n",
    "        q, k, v = self.to_qkv(msa_norm).chunk(3, dim=-1)\n",
    "        query, key, value = [t.reshape(B,N,L,self.h,self.dim) for t in (q, k, v)]\n",
    "\n",
    "        bias, gate = self.to_b(pair_norm), torch.sigmoid(self.to_g(msa_norm))\n",
    "        \n",
    "        m_view = bool_mask.view(B, 1, L, 1, 1)\n",
    "        query, value, key = query.masked_fill(m_view, 0.0), value.masked_fill(m_view, 0.0), key.masked_fill(m_view, 0.0)\n",
    "        \n",
    "        attn = einsum('bnqhd,bnkhd->bqkh', query*self.scaling, key) + bias\n",
    "        mask_2d = bool_mask.unsqueeze(1) | bool_mask.unsqueeze(2)\n",
    "        if mask_2d.any(): attn = attn.masked_fill(mask_2d.view(B, L, L, 1), -1e9)\n",
    "        \n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        \n",
    "        out = einsum('bqkh,bnkhd->bnqhd', attn, value).reshape(B, N, L, -1)\n",
    "        out = gate * out\n",
    "        out = self.to_out(out)\n",
    "        if bool_mask.any(): out = out.masked_fill(bool_mask.view(B, 1, L, 1), 0.0)\n",
    "        return out\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 定义模型 - BiasedAxialAttention\n",
    "# =============================================================================\n",
    "class BiasedAxialAttentionOriginal(nn.Module):\n",
    "    def __init__(self, d_pair, d_bias, n_head, d_hidden, is_row=True):\n",
    "        super().__init__()\n",
    "        self.is_row, self.norm_pair, self.norm_bias = is_row, nn.LayerNorm(d_pair), nn.LayerNorm(d_bias)\n",
    "        self.to_q, self.to_k, self.to_v = nn.Linear(d_pair, n_head*d_hidden, bias=False), nn.Linear(d_pair, n_head*d_hidden, bias=False), nn.Linear(d_pair, n_head*d_hidden, bias=False)\n",
    "        self.to_b, self.to_g, self.to_out = nn.Linear(d_bias, n_head, bias=False), nn.Linear(d_pair, n_head*d_hidden), nn.Linear(n_head*d_hidden, d_pair)\n",
    "        self.scaling, self.h, self.dim = 1/math.sqrt(d_hidden), n_head, d_hidden\n",
    "        self.reset_parameter()\n",
    "    def reset_parameter(self):\n",
    "        for w in [self.to_q.weight, self.to_k.weight, self.to_v.weight]: nn.init.xavier_uniform_(w)\n",
    "        self.to_b = init_lecun_normal(self.to_b)\n",
    "        if hasattr(self.to_g, 'bias') and self.to_g.bias is not None:\n",
    "            nn.init.ones_(self.to_g.bias)\n",
    "    def forward(self, pair, bias, mask=None):\n",
    "        B, L, _, _ = pair.shape\n",
    "        bool_mask = (mask == 0)\n",
    "        mask_2d = bool_mask.unsqueeze(1) | bool_mask.unsqueeze(2)\n",
    "        safe_pair, safe_bias = torch.nan_to_num(pair), torch.nan_to_num(bias)\n",
    "        if self.is_row:\n",
    "            safe_pair, safe_bias = safe_pair.permute(0, 2, 1, 3), safe_bias.permute(0, 2, 1, 3)\n",
    "            mask_2d = mask_2d.permute(0, 2, 1)\n",
    "        pair_norm, bias_norm = self.norm_pair(safe_pair), self.norm_bias(safe_bias)\n",
    "        query, key, value = [t.reshape(B,L,L,self.h,self.dim) for t in (self.to_q(pair_norm), self.to_k(pair_norm), self.to_v(pair_norm))]\n",
    "        bias_h, gate = self.to_b(bias_norm), torch.sigmoid(self.to_g(pair_norm))\n",
    "        query, key = query * self.scaling, key / math.sqrt(L)\n",
    "        attn = einsum('bnihd,bnjhd->bijh', query, key) + bias_h\n",
    "        if mask_2d.any(): attn = attn.masked_fill(mask_2d.unsqueeze(-1), -1e9)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        out = einsum('bijh,bkjhd->bikhd', attn, value).reshape(B, L, L, -1)\n",
    "        out = gate * out\n",
    "        out = self.to_out(out)\n",
    "        if self.is_row: out = out.permute(0, 2, 1, 3)\n",
    "        if mask_2d.any(): out = out.masked_fill(mask_2d.unsqueeze(-1), 0.0)\n",
    "        return out\n",
    "\n",
    "class BiasedAxialAttentionOptimized(nn.Module):\n",
    "    def __init__(self, d_pair, d_bias, n_head, d_hidden, is_row=True):\n",
    "        super().__init__()\n",
    "        self.is_row, self.norm_pair, self.norm_bias = is_row, nn.LayerNorm(d_pair), nn.LayerNorm(d_bias)\n",
    "        self.to_qkv = nn.Linear(d_pair, 3 * n_head * d_hidden, bias=False)\n",
    "        self.to_b, self.to_g, self.to_out = nn.Linear(d_bias, n_head, bias=False), nn.Linear(d_pair, n_head*d_hidden), nn.Linear(n_head*d_hidden, d_pair)\n",
    "        self.scaling, self.h, self.dim = 1/math.sqrt(d_hidden), n_head, d_hidden\n",
    "        self.reset_parameter()\n",
    "    def reset_parameter(self):\n",
    "        nn.init.xavier_uniform_(self.to_qkv.weight)\n",
    "        self.to_b = init_lecun_normal(self.to_b)\n",
    "        if hasattr(self.to_g, 'bias') and self.to_g.bias is not None:\n",
    "            nn.init.ones_(self.to_g.bias)\n",
    "    def forward(self, pair, bias, mask=None):\n",
    "        B, L, _, _ = pair.shape\n",
    "        bool_mask = (mask == 0)\n",
    "        mask_2d = bool_mask.unsqueeze(1) | bool_mask.unsqueeze(2)\n",
    "        safe_pair, safe_bias = torch.nan_to_num(pair), torch.nan_to_num(bias)\n",
    "        if self.is_row:\n",
    "            safe_pair, safe_bias = safe_pair.permute(0, 2, 1, 3), safe_bias.permute(0, 2, 1, 3)\n",
    "            mask_2d = mask_2d.permute(0, 2, 1)\n",
    "        \n",
    "        pair_norm, bias_norm = self.norm_pair(safe_pair), self.norm_bias(safe_bias)\n",
    "        \n",
    "        q, k, v = self.to_qkv(pair_norm).chunk(3, dim=-1)\n",
    "        query, key, value = [t.reshape(B,L,L,self.h,self.dim) for t in (q, k, v)]\n",
    "\n",
    "        bias_h, gate = self.to_b(bias_norm), torch.sigmoid(self.to_g(pair_norm))\n",
    "        query, key = query * self.scaling, key / math.sqrt(L)\n",
    "        \n",
    "        attn = einsum('bnihd,bnjhd->bijh', query, key) + bias_h\n",
    "        if mask_2d.any(): attn = attn.masked_fill(mask_2d.unsqueeze(-1), -1e9)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        out = einsum('bijh,bkjhd->bikhd', attn, value).reshape(B, L, L, -1)\n",
    "        out = gate * out\n",
    "        out = self.to_out(out)\n",
    "        if self.is_row: out = out.permute(0, 2, 1, 3)\n",
    "        if mask_2d.any(): out = out.masked_fill(mask_2d.unsqueeze(-1), 0.0)\n",
    "        return out\n",
    "\n",
    "# =============================================================================\n",
    "# 5. 测试函数\n",
    "# =============================================================================\n",
    "\n",
    "def copy_weights(model_original, model_optimized):\n",
    "    \"\"\"\n",
    "    将原始模型的权重精确地复制到优化模型中。\n",
    "    这个新版本更健壮，能正确处理所有情况。\n",
    "    \"\"\"\n",
    "    # 获取两个模型的 state_dict\n",
    "    orig_sd = model_original.state_dict()\n",
    "    opt_sd = model_optimized.state_dict()\n",
    "    \n",
    "    new_opt_sd = {}\n",
    "    \n",
    "    # 遍历优化模型的每一个参数\n",
    "    for name, param in opt_sd.items():\n",
    "        if name == 'to_kv.weight':\n",
    "            # 从原始模型拼接 k, v 权重\n",
    "            k_w = orig_sd['to_k.weight']\n",
    "            v_w = orig_sd['to_v.weight']\n",
    "            new_opt_sd[name] = torch.cat([k_w, v_w], dim=0)\n",
    "        elif name == 'to_qkv.weight':\n",
    "            # 从原始模型拼接 q, k, v 权重\n",
    "            q_w = orig_sd['to_q.weight']\n",
    "            k_w = orig_sd['to_k.weight']\n",
    "            v_w = orig_sd['to_v.weight']\n",
    "            new_opt_sd[name] = torch.cat([q_w, k_w, v_w], dim=0)\n",
    "        elif name in orig_sd:\n",
    "            # 如果在原始模型中存在同名参数，直接复制\n",
    "            new_opt_sd[name] = orig_sd[name]\n",
    "        else:\n",
    "            # 如果没有，保留优化模型自己的参数（这种情况不应发生）\n",
    "            new_opt_sd[name] = param\n",
    "            \n",
    "    # 加载构建好的新 state_dict\n",
    "    model_optimized.load_state_dict(new_opt_sd)\n",
    "\n",
    "\n",
    "def check_consistency(model_original, model_optimized, inputs, device, name=\"\"):\n",
    "    \"\"\"检查原始模型和优化模型的输出是否一致\"\"\"\n",
    "    print(f\"--- 正在检查 {name} 的一致性 ---\")\n",
    "    \n",
    "    # 将模型和数据移到设备\n",
    "    model_original.to(device)\n",
    "    model_optimized.to(device)\n",
    "    \n",
    "    # **关键步骤：在比较前复制权重**\n",
    "    copy_weights(model_original, model_optimized)\n",
    "    \n",
    "    inputs_dev = [t.to(device) for t in inputs if t is not None]\n",
    "\n",
    "    # 设置为评估模式\n",
    "    model_original.eval()\n",
    "    model_optimized.eval()\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            output_original = model_original(*inputs_dev)\n",
    "            output_optimized = model_optimized(*inputs_dev)\n",
    "\n",
    "        # 比较输出\n",
    "        are_close = torch.allclose(output_original, output_optimized, atol=1e-5, rtol=1e-4)\n",
    "        max_diff = (output_original - output_optimized).abs().max().item()\n",
    "\n",
    "        if are_close:\n",
    "            print(f\"✅ PASS: 输出一致。最大差异: {max_diff:.6e}\")\n",
    "        else:\n",
    "            print(f\"❌ FAIL: 输出不一致。最大差异: {max_diff:.6e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: 在检查一致性时发生错误: {e}\")\n",
    "    \n",
    "    print(\"-\" * (20 + len(name)))\n",
    "\n",
    "\n",
    "def benchmark(model, inputs, device, iterations=50):\n",
    "    model.to(device)\n",
    "    model.train() # 确保模型处于训练模式以进行反向传播\n",
    "    inputs = [t.to(device) for t in inputs if t is not None]\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        output = model(*inputs)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        model.zero_grad()\n",
    "    if device.type == 'cuda': torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    if device.type == 'cuda': torch.cuda.reset_peak_memory_stats(device)\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        output = model(*inputs)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        model.zero_grad()\n",
    "    if device.type == 'cuda': torch.cuda.synchronize()\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    avg_time_ms = (end_time - start_time) / iterations * 1000\n",
    "    peak_memory_mb = torch.cuda.max_memory_allocated(device) / (1024 * 1024) if device.type == 'cuda' else 0\n",
    "    return avg_time_ms, peak_memory_mb\n",
    "\n",
    "# =============================================================================\n",
    "# 6. 主执行逻辑\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 参数 (调整为更能体现优化效果的真实负载) ---\n",
    "    B, N, L, D_MSA, D_PAIR = 4, 64, 256, 64, 32\n",
    "    N_HEAD, D_HIDDEN = 4, 16\n",
    "    ITERATIONS = 50\n",
    "\n",
    "    # --- 环境 ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"正在使用设备: {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"设备名称: {torch.cuda.get_device_name(device)}\")\n",
    "        if not hasattr(F, 'scaled_dot_product_attention'):\n",
    "            print(\"\\n警告: PyTorch 版本过低，不支持 SDPA，Attention 模块的优化效果将不明显。\")\n",
    "            AttentionOptimized = AttentionOriginal\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"测试参数 (较大负载):\")\n",
    "    print(f\"  Batch Size: {B}, MSA Depth: {N}, Seq Len: {L}\")\n",
    "    print(f\"  D_MSA: {D_MSA}, D_PAIR: {D_PAIR}\")\n",
    "    print(f\"  Heads: {N_HEAD}, Hidden Dim: {D_HIDDEN}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # --- 输入数据 ---\n",
    "    query = torch.randn(B, L, D_MSA)\n",
    "    key = torch.randn(B, L, D_MSA)\n",
    "    msa = torch.randn(B, N, L, D_MSA)\n",
    "    pair = torch.randn(B, L, L, D_PAIR)\n",
    "    bias_for_attn = torch.randn(B, L, L, N_HEAD)\n",
    "    bias_for_axial = torch.randn(B, L, L, N_HEAD)\n",
    "    mask = torch.ones(B, L)\n",
    "    mask[:, -L//4:] = 0 # Mask last quarter\n",
    "\n",
    "    # --- 模型和测试配置 ---\n",
    "    test_configs = {\n",
    "        \"Attention\": {\n",
    "            \"models\": (AttentionOriginal(D_MSA, D_MSA, N_HEAD, D_HIDDEN, D_MSA), \n",
    "                       AttentionOptimized(D_MSA, D_MSA, N_HEAD, D_HIDDEN, D_MSA)),\n",
    "            \"inputs\": (query, key, key, mask)\n",
    "        },\n",
    "        \"AttentionWithBias\": {\n",
    "            \"models\": (AttentionWithBiasOriginal(D_MSA, N_HEAD, N_HEAD, D_HIDDEN), \n",
    "                       AttentionWithBiasOptimized(D_MSA, N_HEAD, N_HEAD, D_HIDDEN)),\n",
    "            \"inputs\": (query, bias_for_attn, mask)\n",
    "        },\n",
    "        \"MSARowAttention\": {\n",
    "            \"models\": (MSARowAttentionWithBiasOriginal(D_MSA, D_PAIR, N_HEAD, D_HIDDEN),\n",
    "                       MSARowAttentionWithBiasOptimized(D_MSA, D_PAIR, N_HEAD, D_HIDDEN)),\n",
    "            \"inputs\": (msa, pair, mask)\n",
    "        },\n",
    "        \"BiasedAxialAttention\": {\n",
    "            \"models\": (BiasedAxialAttentionOriginal(D_PAIR, N_HEAD, N_HEAD, D_HIDDEN),\n",
    "                       BiasedAxialAttentionOptimized(D_PAIR, N_HEAD, N_HEAD, D_HIDDEN)),\n",
    "            \"inputs\": (pair, bias_for_axial, mask)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, config in test_configs.items():\n",
    "        original_model, optimized_model = config[\"models\"]\n",
    "        inputs = config[\"inputs\"]\n",
    "        \n",
    "        # 1. 检查一致性\n",
    "        check_consistency(original_model, optimized_model, inputs, device, name)\n",
    "        \n",
    "        # 2. 运行性能基准测试\n",
    "        print(f\"--- 正在进行 {name} 的性能基准测试 ---\")\n",
    "        print(\"  - 原始版本...\")\n",
    "        orig_t, orig_m = benchmark(original_model, inputs, device, ITERATIONS)\n",
    "        \n",
    "        print(\"  - 优化版本...\")\n",
    "        opt_t, opt_m = benchmark(optimized_model, inputs, device, ITERATIONS)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"orig_t\": orig_t, \"orig_m\": orig_m,\n",
    "            \"opt_t\": opt_t, \"opt_m\": opt_m,\n",
    "            \"time_gain\": (orig_t - opt_t) / orig_t * 100,\n",
    "            \"mem_gain\": (orig_m - opt_m) / orig_m * 100 if orig_m > 0 else 0\n",
    "        }\n",
    "\n",
    "    # --- 打印结果 ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"基准测试最终结果\")\n",
    "    print(\"=\"*80)\n",
    "    header = f\"| {'模块名':<22} | {'版本':<8} | {'时间 (ms/iter)':<16} | {'内存 (MB)':<12} | {'提升':<10} |\"\n",
    "    print(header)\n",
    "    print(f\"|{'-'*24}|{'-'*10}|{'-'*18}|{'-'*14}|{'-'*12}|\")\n",
    "    \n",
    "    for name, res in results.items():\n",
    "        print(f\"| {name:<22} | {'原始':<8} | {res['orig_t']:<16.3f} | {res['orig_m']:<12.2f} | {'-':<10} |\")\n",
    "        print(f\"| {'':<22} | {'优化':<8} | {res['opt_t']:<16.3f} | {res['opt_m']:<12.2f} | {res['time_gain']:>6.2f}% (T) |\")\n",
    "        if device.type == 'cuda':\n",
    "            print(f\"| {'':<22} | {'':<8} | {'':<16} | {'':<12} | {res['mem_gain']:>6.2f}% (M) |\")\n",
    "        print(f\"|{'-'*24}|{'-'*10}|{'-'*18}|{'-'*14}|{'-'*12}|\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1de33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "测试所有 Attention 模块的梯度\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "1. 测试 FeedForwardLayer\n",
      "==================================================\n",
      "Epoch 1: Loss = 1.198163\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "linear2.weight                           | (64, 256)            |  7.948e-01 |     0 |     0 | 1000.0% | [-4.731e-02, 4.004e-02]\n",
      "linear1.weight                           | (256, 64)            |  3.954e-01 |     0 |     0 | 250.0% | [-2.223e-02, 1.733e-02]\n",
      "linear2.bias                             | (64,)                |  1.308e-01 |     0 |     0 |   0.0% | [-5.037e-02, 3.718e-02]\n",
      "linear1.bias                             | (256,)               |  5.741e-02 |     0 |     0 |   3.9% | [-9.596e-03, 1.318e-02]\n",
      "norm.bias                                | (64,)                |  3.470e-02 |     0 |     0 |   0.0% | [-9.698e-03, 9.952e-03]\n",
      "norm.weight                              | (64,)                |  3.428e-02 |     0 |     0 |   0.0% | [-6.590e-03, 1.181e-02]\n",
      "Epoch 2: Loss = 1.103339\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "linear2.weight                           | (64, 256)            |  7.516e-01 |     0 |     0 | 1200.0% | [-3.564e-02, 3.384e-02]\n",
      "linear1.weight                           | (256, 64)            |  3.651e-01 |     0 |     0 | 300.0% | [-1.942e-02, 1.856e-02]\n",
      "linear2.bias                             | (64,)                |  1.206e-01 |     0 |     0 |   0.0% | [-4.621e-02, 3.432e-02]\n",
      "linear1.bias                             | (256,)               |  5.202e-02 |     0 |     0 |   4.7% | [-1.117e-02, 9.291e-03]\n",
      "norm.bias                                | (64,)                |  3.219e-02 |     0 |     0 |   0.0% | [-1.050e-02, 9.726e-03]\n",
      "norm.weight                              | (64,)                |  2.547e-02 |     0 |     0 |   0.0% | [-7.442e-03, 8.178e-03]\n",
      "Epoch 3: Loss = 1.018691\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "linear2.weight                           | (64, 256)            |  7.185e-01 |     0 |     0 | 1000.0% | [-4.713e-02, 3.804e-02]\n",
      "linear1.weight                           | (256, 64)            |  3.490e-01 |     0 |     0 | 250.0% | [-1.678e-02, 1.819e-02]\n",
      "linear2.bias                             | (64,)                |  1.119e-01 |     0 |     0 |   0.0% | [-4.651e-02, 3.200e-02]\n",
      "linear1.bias                             | (256,)               |  4.850e-02 |     0 |     0 |   3.9% | [-1.125e-02, 1.088e-02]\n",
      "norm.bias                                | (64,)                |  2.845e-02 |     0 |     0 |   0.0% | [-8.156e-03, 7.551e-03]\n",
      "norm.weight                              | (64,)                |  2.610e-02 |     0 |     0 |   0.0% | [-9.944e-03, 5.921e-03]\n",
      "\n",
      "==================================================\n",
      "2. 测试 Attention\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 313\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 313\u001b[0m     \u001b[43mtest_all_attention_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mtest_all_attention_modules\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    118\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 119\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(out[mask], target[mask])\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/WORK/.conda/envs/SE3nv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/WORK/.conda/envs/SE3nv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/WORK/PUBLIC/lulei_work/xjt/Protein_design/RFdiffusion/mytest/Attention_module.py:63\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n\u001b[1;32m     62\u001b[0m attn \u001b[38;5;241m=\u001b[39m einsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbqhd,bkhd->bhqk\u001b[39m\u001b[38;5;124m'\u001b[39m, q, k)\n\u001b[0;32m---> 63\u001b[0m mask \u001b[38;5;241m=\u001b[39m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m     64\u001b[0m mask_2d \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     66\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mmasked_fill(mask_2d\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from opt_einsum import contract as einsum\n",
    "from rfdiffusion.util_module import init_lecun_normal\n",
    "from mytest.Attention_module import *\n",
    "\n",
    "def print_all_gradients(model, epoch=None):\n",
    "    \"\"\"打印模型中所有可训练参数的梯度信息\"\"\"\n",
    "    if epoch is not None:\n",
    "        print(f\"\\n=== Epoch {epoch} 梯度信息 ===\")\n",
    "    else:\n",
    "        print(f\"\\n=== 当前梯度信息 ===\")\n",
    "    \n",
    "    # 收集所有梯度信息\n",
    "    grad_data = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad = param.grad.data\n",
    "            grad_info = {\n",
    "                \"name\": name,\n",
    "                \"shape\": tuple(grad.shape),\n",
    "                \"dtype\": str(grad.dtype),\n",
    "                \"norm\": grad.norm().item(),\n",
    "                \"min\": grad.min().item(),\n",
    "                \"max\": grad.max().item(),\n",
    "                \"mean\": grad.mean().item(),\n",
    "                \"nan\": torch.isnan(grad).sum().item(),\n",
    "                \"inf\": torch.isinf(grad).sum().item(),\n",
    "                \"zero\": (grad == 0).sum().item()\n",
    "            }\n",
    "            grad_data.append(grad_info)\n",
    "    \n",
    "    # 按梯度范数排序（从大到小）\n",
    "    grad_data.sort(key=lambda x: x[\"norm\"], reverse=True)\n",
    "    \n",
    "    # 打印表格头\n",
    "    print(f\"{'参数名称':<40} | {'形状':<20} | {'范数':>10} | {'NaN':>5} | {'Inf':>5} | {'零值%':>6} | {'范围'}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # 打印每个参数的梯度信息\n",
    "    for info in grad_data:\n",
    "        if len(grad_data) > 0:\n",
    "            zero_percent = info[\"zero\"] / (info['shape'][0] if len(info['shape']) > 0 else 1) * 100\n",
    "        else:\n",
    "            zero_percent = 0\n",
    "        range_str = f\"[{info['min']:.3e}, {info['max']:.3e}]\"\n",
    "        \n",
    "        # 高亮异常梯度\n",
    "        if info[\"nan\"] > 0 or info[\"inf\"] > 0:\n",
    "            highlight = \"\\033[91m\"  # 红色\n",
    "            reset = \"\\033[0m\"\n",
    "        else:\n",
    "            highlight = reset = \"\"\n",
    "        \n",
    "        print(f\"{highlight}{info['name']:<40} | {str(info['shape']):<20} | \"\n",
    "            f\"{info['norm']:>10.3e} | \"\n",
    "            f\"{info['nan']:>5} | \"\n",
    "            f\"{info['inf']:>5} | \"\n",
    "            f\"{zero_percent:>5.1f}% | \"\n",
    "            f\"{range_str}{reset}\")\n",
    "\n",
    "def test_all_attention_modules():\n",
    "    torch.manual_seed(42)\n",
    "    B, L = 2, 5\n",
    "    d_model, d_pair, d_bias = 64, 1, 32\n",
    "    n_head, d_hidden = 4, 16\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"测试所有 Attention 模块的梯度\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. 测试 FeedForwardLayer\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"1. 测试 FeedForwardLayer\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    data = torch.randn(B, L, d_model)\n",
    "    mask = torch.ones(B, L, dtype=torch.bool)\n",
    "    mask[0, 3:] = 0\n",
    "    mask[1, :2] = 0\n",
    "    \n",
    "    data[0, 3:] = float('nan')\n",
    "    data[1, :2] = float('nan')\n",
    "    \n",
    "    ffn = FeedForwardLayer(d_model, 4)\n",
    "    optimizer = torch.optim.Adam(ffn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(data)\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = ffn(data)\n",
    "        loss = F.mse_loss(out[mask], target[mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(ffn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 2. 测试 Attention\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"2. 测试 Attention\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    data = torch.randn(B, L, d_model)\n",
    "    mask = torch.ones(B, L, dtype=torch.bool)\n",
    "    mask[0, 3:] = 0\n",
    "    mask[1, :2] = 0\n",
    "    \n",
    "    data[0, 3:] = float('nan')\n",
    "    data[1, :2] = float('nan')\n",
    "    \n",
    "    attn = Attention(d_model, d_model, n_head, d_hidden, d_model)\n",
    "    optimizer = torch.optim.Adam(attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(data)\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = attn(data, data, data)\n",
    "        loss = F.mse_loss(out[mask], target[mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 3. 测试 AttentionWithBias\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"3. 测试 AttentionWithBias\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    data = torch.randn(B, L, d_model)\n",
    "    bias = torch.randn(B, L, L, d_bias)\n",
    "    mask = torch.ones(B, L, dtype=torch.bool)\n",
    "    mask[0, 3:] = 0\n",
    "    mask[1, :2] = 0\n",
    "    \n",
    "    data[0, 3:] = float('nan')\n",
    "    data[1, :2] = float('nan')\n",
    "    \n",
    "    attn_bias = AttentionWithBias(d_model, d_bias, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(attn_bias.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(data)\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = attn_bias(data, bias)\n",
    "        loss = F.mse_loss(out[mask], target[mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(attn_bias, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 4. 测试 SequenceWeight\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"4. 测试 SequenceWeight\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    msa = torch.randn(B, N, L, d_model)\n",
    "    # 在MSA的pad位置设置为0\n",
    "    msa_mask = mask.unsqueeze(1).expand(B, N, L)  # [B, N, L]\n",
    "    msa[~msa_mask] = 0.0\n",
    "\n",
    "    seq_weight = SequenceWeight(d_model, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(seq_weight.parameters(), lr=0.001)\n",
    "    target = torch.randn(B, N, L, n_head, 1)\n",
    "    target_mask = mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1).expand_as(target)\n",
    "    target[~target_mask] = 0.0\n",
    "\n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = seq_weight(msa, mask=mask)\n",
    "        \n",
    "        # 克隆输出以避免就地修改\n",
    "        out_cloned = out.clone()\n",
    "        print(out_cloned)\n",
    "        \n",
    "        loss = F.mse_loss(out_cloned[target_mask], target[target_mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(seq_weight, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 5. 测试 MSARowAttentionWithBias\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"5. 测试 MSARowAttentionWithBias\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    B, N, L = 2, 3, 5\n",
    "    msa = torch.randn(B, N, L, d_model)\n",
    "    pair = torch.randn(B, L, L, d_pair)\n",
    "    mask = torch.ones(B, L, dtype=torch.bool)\n",
    "    mask[0, 3:] = 0\n",
    "    mask[1, :2] = 0\n",
    "    \n",
    "    msa[0, :, 3:] = float('nan')\n",
    "    msa[1, :, :2] = float('nan')\n",
    "    \n",
    "    msa_row_attn = MSARowAttentionWithBias(d_model, d_pair, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(msa_row_attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(msa)\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = msa_row_attn(msa, pair)\n",
    "        # 创建目标mask\n",
    "        target_mask = mask.unsqueeze(1).unsqueeze(-1).expand_as(out)\n",
    "        loss = F.mse_loss(out[target_mask], target[target_mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(msa_row_attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 6. 测试 MSAColAttention\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"6. 测试 MSAColAttention\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    B, N, L = 2, 3, 5\n",
    "    msa = torch.randn(B, N, L, d_model)\n",
    "    mask = torch.ones(B, L, dtype=torch.bool)\n",
    "    mask[0, 3:] = 0\n",
    "    mask[1, :2] = 0\n",
    "    \n",
    "    msa[0, :, 3:] = float('nan')\n",
    "    msa[1, :, :2] = float('nan')\n",
    "    \n",
    "    msa_col_attn = MSAColAttention(d_model, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(msa_col_attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(msa)\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = msa_col_attn(msa)\n",
    "        # 创建目标mask\n",
    "        target_mask = mask.unsqueeze(1).unsqueeze(-1).expand_as(out)\n",
    "        loss = F.mse_loss(out[target_mask], target[target_mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(msa_col_attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 7. 测试 MSAColGlobalAttention\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"7. 测试 MSAColGlobalAttention\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    B, N, L = 2, 3, 5\n",
    "    msa = torch.randn(B, N, L, d_model)\n",
    "    mask = torch.ones(B, L, dtype=torch.bool)\n",
    "    mask[0, 3:] = 0\n",
    "    mask[1, :2] = 0\n",
    "    \n",
    "    msa[0, :, 3:] = float('nan')\n",
    "    msa[1, :, :2] = float('nan')\n",
    "    \n",
    "    msa_global_attn = MSAColGlobalAttention(d_model, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(msa_global_attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(msa)\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = msa_global_attn(msa)\n",
    "        # 创建目标mask\n",
    "        target_mask = mask.unsqueeze(1).unsqueeze(-1).expand_as(out)\n",
    "        loss = F.mse_loss(out[target_mask], target[target_mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(msa_global_attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 8. 测试 BiasedAxialAttention\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"8. 测试 BiasedAxialAttention\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    B, L = 2, 5\n",
    "    pair = torch.randn(B, L, L, d_pair)\n",
    "    bias = torch.randn(B, L, L, d_bias)\n",
    "    mask = torch.ones(B, L, dtype=torch.bool)\n",
    "    mask[0, 3:] = 0\n",
    "    mask[1, :2] = 0\n",
    "    \n",
    "    # 在无效位置添加NaN\n",
    "    pair[0, 3:, :] = float('nan')\n",
    "    pair[0, :, 3:] = float('nan')\n",
    "    pair[1, :2, :] = float('nan')\n",
    "    pair[1, :, :2] = float('nan')\n",
    "    \n",
    "    axial_attn = BiasedAxialAttention(d_pair, d_bias, n_head, d_hidden, is_row=True)\n",
    "    optimizer = torch.optim.Adam(axial_attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(pair)\n",
    "    \n",
    "    # 创建布尔掩码\n",
    "    loss_mask = torch.zeros(B, L, L, dtype=torch.bool)\n",
    "    loss_mask[0, :3, :3] = True\n",
    "    loss_mask[1, 2:, 2:] = True\n",
    "    loss_mask = loss_mask.unsqueeze(-1).expand_as(pair)\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = axial_attn(pair, bias)\n",
    "        loss = F.mse_loss(out[loss_mask], target[loss_mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(axial_attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"所有模块测试完成!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_all_attention_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b212afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "测试所有 Attention 模块 - 使用 padding=0\n",
      "====================================================================================================\n",
      "Mask shape: torch.Size([2, 8])\n",
      "Valid positions - Batch 0: 5, Batch 1: 3\n",
      "\n",
      "============================================================\n",
      "1. 测试 FeedForwardLayer\n",
      "============================================================\n",
      "Epoch 1: Loss = 1.179109\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm.weight                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "norm.bias                                | (16,)                |  6.382e-02 |     0 |     0 |   0.0% | [-2.606e-02, 2.799e-02]\n",
      "\u001b[91mlinear1.weight                           | (64, 16)             |        nan |  1024 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "linear1.bias                             | (64,)                |  9.206e-02 |     0 |     0 |   3.1% | [-1.496e-02, 3.071e-02]\n",
      "\u001b[91mlinear2.weight                           | (16, 64)             |        nan |  1024 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "linear2.bias                             | (16,)                |  1.978e-01 |     0 |     0 |   0.0% | [-7.991e-02, 8.600e-02]\n",
      "Epoch 2: Loss = nan\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm.weight                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mlinear1.weight                           | (64, 16)             |        nan |  1024 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mlinear1.bias                             | (64,)                |        nan |    64 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mlinear2.weight                           | (16, 64)             |        nan |  1024 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mlinear2.bias                             | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "Epoch 3: Loss = nan\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm.weight                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mlinear1.weight                           | (64, 16)             |        nan |  1024 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mlinear1.bias                             | (64,)                |        nan |    64 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mlinear2.weight                           | (16, 64)             |        nan |  1024 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mlinear2.bias                             | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\n",
      "============================================================\n",
      "2. 测试 Attention\n",
      "============================================================\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Epoch 1: Loss = nan\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Epoch 2: Loss = nan\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Epoch 3: Loss = nan\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\n",
      "============================================================\n",
      "3. 测试 AttentionWithBias\n",
      "============================================================\n",
      "Epoch 1: Loss = nan\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_in.weight                           | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_in.bias                             | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_bias.weight                         | (32,)                |        nan |    32 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_bias.bias                           | (32,)                |        nan |    32 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_b.weight                              | (4, 32)              |        nan |   128 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "Epoch 2: Loss = nan\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_in.weight                           | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_in.bias                             | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_bias.weight                         | (32,)                |        nan |    32 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_bias.bias                           | (32,)                |        nan |    32 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_b.weight                              | (4, 32)              |        nan |   128 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "Epoch 3: Loss = nan\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_in.weight                           | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_in.bias                             | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_bias.weight                         | (32,)                |        nan |    32 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_bias.bias                           | (32,)                |        nan |    32 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_b.weight                              | (4, 32)              |        nan |   128 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\n",
      "============================================================\n",
      "4. 测试 SequenceWeight\n",
      "============================================================\n",
      "Epoch 1: Loss = 3.024704\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mto_query.weight                          | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_query.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_key.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_key.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "Epoch 2: Loss = nan\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mto_query.weight                          | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_query.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_key.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_key.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "Epoch 3: Loss = nan\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mto_query.weight                          | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_query.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_key.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_key.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\n",
      "============================================================\n",
      "5. 测试 MSARowAttentionWithBias\n",
      "============================================================\n",
      "msa shape: torch.Size([2, 1, 8, 16]) pair shape: torch.Size([2, 8, 8, 24])\n",
      "torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 4, 8, 8]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 1, 8, 16])\n",
      "gate shape: torch.Size([2, 1, 8, 16])\n",
      "torch.Size([2, 1, 8, 16])\n",
      "Epoch 1: Loss = 0.870247\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (16, 16)             |  3.552e-01 |     0 |     0 |   0.0% | [-6.696e-02, 8.161e-02]\n",
      "to_out.bias                              | (16,)                |  1.670e-01 |     0 |     0 |   0.0% | [-9.407e-02, 6.466e-02]\n",
      "norm_msa.weight                          | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "norm_msa.bias                            | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "norm_pair.weight                         | (24,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "norm_pair.bias                           | (24,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_query.weight               | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_query.bias                 | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_key.weight                 | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_key.bias                   | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_q.weight                              | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_k.weight                              | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_v.weight                              | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_b.weight                              | (4, 24)              |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_g.weight                              | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_g.bias                                | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "msa shape: torch.Size([2, 1, 8, 16]) pair shape: torch.Size([2, 8, 8, 24])\n",
      "torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 4, 8, 8]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 1, 8, 16])\n",
      "gate shape: torch.Size([2, 1, 8, 16])\n",
      "torch.Size([2, 1, 8, 16])\n",
      "Epoch 2: Loss = 0.865870\n",
      "tensor([[[[-3.2356e-05, -1.1799e-05,  3.1323e-05,  1.1132e-05, -5.9350e-06,\n",
      "           -1.7901e-05,  3.2816e-05,  2.2994e-05,  2.4128e-05,  1.8008e-05,\n",
      "           -1.2450e-05, -7.7744e-06,  1.9794e-06, -1.7161e-05, -1.4343e-05,\n",
      "           -2.2661e-05],\n",
      "          [-1.6657e-05, -4.0860e-05,  4.9207e-05,  7.7458e-06, -2.9160e-05,\n",
      "           -7.7977e-05, -1.0973e-05,  7.7743e-05, -3.8896e-06,  3.4336e-05,\n",
      "           -1.0779e-05,  7.3454e-05,  1.8786e-05,  1.4923e-05, -5.5393e-05,\n",
      "           -3.0507e-05],\n",
      "          [-5.6686e-05, -6.2092e-05,  3.2166e-05, -8.4826e-06, -8.5170e-06,\n",
      "           -7.9802e-05,  7.8661e-06,  6.2771e-05,  2.2411e-05,  1.0449e-05,\n",
      "            2.3135e-06,  5.0657e-05,  1.9434e-05,  3.0576e-05, -8.7318e-06,\n",
      "           -1.4332e-05],\n",
      "          [ 2.5525e-06, -3.9861e-05,  7.2835e-05, -1.3284e-05, -2.6417e-05,\n",
      "           -9.6189e-05, -1.1981e-05,  1.1526e-05,  2.6770e-05,  3.8153e-05,\n",
      "            1.1509e-05,  5.9763e-05, -1.3419e-05, -1.4321e-05, -2.3401e-05,\n",
      "            1.5764e-05],\n",
      "          [-1.0723e-05, -4.1557e-05,  6.8410e-06, -5.0368e-05, -3.5740e-05,\n",
      "           -3.4978e-05,  5.0546e-05,  5.1021e-05,  5.3433e-05,  1.8581e-05,\n",
      "            7.2528e-05,  5.2431e-05,  1.6945e-07, -2.7995e-05, -5.4889e-05,\n",
      "           -4.9300e-05],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [-4.3166e-05,  5.7990e-05,  4.8161e-05,  7.3709e-06,  4.1515e-05,\n",
      "            1.5064e-05, -8.4757e-05, -1.4371e-05, -2.5803e-05,  7.6561e-05,\n",
      "           -1.5422e-05, -2.4143e-05, -3.1837e-05,  1.9886e-05, -2.4429e-05,\n",
      "           -2.6201e-06],\n",
      "          [-2.5569e-05,  4.8486e-05,  7.4575e-05,  1.2362e-05,  3.5956e-05,\n",
      "            4.5848e-05, -6.2189e-06, -6.3680e-05,  1.6260e-05,  3.5984e-05,\n",
      "           -5.7156e-05, -3.1295e-05, -1.9044e-05, -6.2071e-06, -5.0266e-05,\n",
      "           -1.0036e-05],\n",
      "          [ 5.1643e-05,  1.0175e-04,  3.0316e-05, -5.6432e-05,  3.1658e-05,\n",
      "            9.4950e-05, -6.6304e-05, -4.8423e-05, -2.7023e-05, -4.9047e-05,\n",
      "            5.9639e-05, -6.2549e-06, -1.5819e-05, -2.8578e-05, -6.2963e-05,\n",
      "           -9.1143e-06],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00]]]])\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (16, 16)             |  3.635e-01 |     0 |     0 |   0.0% | [-7.095e-02, 7.440e-02]\n",
      "to_out.bias                              | (16,)                |  1.658e-01 |     0 |     0 |   0.0% | [-9.360e-02, 6.406e-02]\n",
      "to_v.weight                              | (16, 16)             |  2.449e-03 |     0 |     0 |   0.0% | [-3.745e-04, 3.630e-04]\n",
      "to_b.weight                              | (4, 24)              |  2.084e-03 |     0 |     0 |   0.0% | [-5.228e-04, 4.898e-04]\n",
      "norm_msa.weight                          | (16,)                |  1.143e-03 |     0 |     0 |   0.0% | [-5.240e-04, -1.679e-05]\n",
      "norm_msa.bias                            | (16,)                |  8.349e-04 |     0 |     0 |   0.0% | [-3.959e-04, 5.460e-04]\n",
      "to_g.weight                              | (16, 16)             |  6.988e-04 |     0 |     0 |   0.0% | [-1.624e-04, 1.356e-04]\n",
      "norm_pair.weight                         | (24,)                |  5.677e-04 |     0 |     0 |   0.0% | [-3.575e-04, 1.611e-04]\n",
      "to_g.bias                                | (16,)                |  2.701e-04 |     0 |     0 |   0.0% | [-1.018e-04, -3.652e-05]\n",
      "norm_pair.bias                           | (24,)                |  1.005e-10 |     0 |     0 |   4.2% | [-4.002e-11, 4.729e-11]\n",
      "seq_weight.to_query.weight               | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_query.bias                 | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_key.weight                 | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_key.bias                   | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_q.weight                              | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_k.weight                              | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "msa shape: torch.Size([2, 1, 8, 16]) pair shape: torch.Size([2, 8, 8, 24])\n",
      "torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 1, 8, 4, 4]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 4, 8, 8]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 4]) torch.Size([2, 4, 8, 8])\n",
      "torch.Size([2, 1, 8, 16])\n",
      "gate shape: torch.Size([2, 1, 8, 16])\n",
      "torch.Size([2, 1, 8, 16])\n",
      "Epoch 3: Loss = 0.861388\n",
      "tensor([[[[-1.0415e-04,  1.1110e-05,  1.3688e-04,  5.6335e-05,  1.2046e-05,\n",
      "           -6.7722e-05,  9.2151e-05,  4.2082e-05,  6.9383e-05,  2.4465e-05,\n",
      "           -4.5605e-05, -3.0862e-05,  2.5461e-05, -8.3034e-05, -8.2015e-05,\n",
      "           -5.6518e-05],\n",
      "          [ 3.4907e-05, -9.3403e-05,  4.1112e-05, -5.7104e-05, -1.1685e-04,\n",
      "           -1.2673e-04,  3.1592e-05,  1.8206e-04, -8.6656e-05,  2.9107e-05,\n",
      "           -2.0532e-05,  2.9247e-04,  1.2065e-04,  5.8355e-05, -1.3353e-04,\n",
      "           -1.5545e-04],\n",
      "          [-1.3909e-04, -1.2113e-04,  1.0269e-04, -2.7393e-05, -2.7684e-05,\n",
      "           -1.9426e-04,  6.2079e-05,  1.3102e-04,  8.9149e-05,  4.5476e-05,\n",
      "            3.3301e-05,  1.0596e-04,  1.9304e-05,  2.5118e-05, -5.7184e-05,\n",
      "           -4.7351e-05],\n",
      "          [-2.3224e-05, -1.1960e-04,  1.9212e-04, -7.7008e-06, -8.2985e-05,\n",
      "           -2.9825e-04, -1.7596e-05,  4.1242e-05,  8.1753e-05,  1.0327e-04,\n",
      "           -1.0744e-05,  1.8767e-04,  1.1377e-06, -4.5718e-05, -4.5970e-05,\n",
      "            4.4594e-05],\n",
      "          [-4.3840e-05, -1.4196e-04,  3.1523e-05, -1.6204e-04, -9.2864e-05,\n",
      "           -1.3504e-04,  1.1323e-04,  1.8204e-04,  1.9172e-04,  1.1133e-04,\n",
      "            2.0369e-04,  1.1492e-04, -5.0102e-05, -4.4931e-05, -1.5337e-04,\n",
      "           -1.2432e-04],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [-9.1288e-05,  1.6290e-04,  1.0546e-04,  2.2255e-05,  1.0050e-04,\n",
      "            1.0595e-04, -1.2765e-04, -1.1606e-04, -1.4638e-04,  1.7487e-04,\n",
      "            6.2827e-06, -1.0351e-04,  4.2806e-05,  3.3211e-06, -1.1418e-04,\n",
      "           -2.5277e-05],\n",
      "          [-8.5906e-05,  1.4864e-04,  2.3067e-04,  1.6662e-05,  1.1193e-04,\n",
      "            1.4355e-04, -2.0156e-05, -1.8213e-04,  5.8602e-05,  1.4304e-04,\n",
      "           -1.8664e-04, -1.1867e-04, -9.1005e-05, -8.4837e-06, -1.1961e-04,\n",
      "           -4.0482e-05],\n",
      "          [ 1.5214e-04,  3.1176e-04,  8.6757e-05, -1.9578e-04,  8.3148e-05,\n",
      "            3.0910e-04, -1.7537e-04, -1.6341e-04, -8.8509e-05, -1.3541e-04,\n",
      "            2.1318e-04, -3.4304e-05, -5.0770e-05, -8.7373e-05, -1.8879e-04,\n",
      "           -3.6367e-05],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00]]]])\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (16, 16)             |  3.643e-01 |     0 |     0 |   0.0% | [-7.434e-02, 7.462e-02]\n",
      "to_out.bias                              | (16,)                |  1.646e-01 |     0 |     0 |   0.0% | [-9.305e-02, 6.357e-02]\n",
      "to_v.weight                              | (16, 16)             |  4.812e-03 |     0 |     0 |   0.0% | [-7.717e-04, 7.126e-04]\n",
      "to_b.weight                              | (4, 24)              |  3.903e-03 |     0 |     0 |   0.0% | [-9.095e-04, 8.903e-04]\n",
      "norm_msa.weight                          | (16,)                |  2.288e-03 |     0 |     0 |   0.0% | [-1.063e-03, -6.690e-05]\n",
      "norm_msa.bias                            | (16,)                |  1.790e-03 |     0 |     0 |   0.0% | [-9.225e-04, 1.103e-03]\n",
      "to_g.weight                              | (16, 16)             |  1.425e-03 |     0 |     0 |   0.0% | [-3.228e-04, 3.227e-04]\n",
      "norm_pair.weight                         | (24,)                |  1.076e-03 |     0 |     0 |   0.0% | [-6.214e-04, 3.237e-04]\n",
      "to_g.bias                                | (16,)                |  5.492e-04 |     0 |     0 |   0.0% | [-2.053e-04, -5.383e-05]\n",
      "norm_pair.bias                           | (24,)                |  1.447e-10 |     0 |     0 |   0.0% | [-7.640e-11, 4.275e-11]\n",
      "seq_weight.to_query.weight               | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_query.bias                 | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_key.weight                 | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "seq_weight.to_key.bias                   | (16,)                |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_q.weight                              | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "to_k.weight                              | (16, 16)             |  0.000e+00 |     0 |     0 | 100.0% | [0.000e+00, 0.000e+00]\n",
      "\n",
      "============================================================\n",
      "6. 测试 MSAColAttention\n",
      "============================================================\n",
      "Epoch 1: Loss = 1.165861\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_msa.weight                          | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_msa.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "to_out.bias                              | (16,)                |  2.295e-01 |     0 |     0 |   0.0% | [-1.086e-01, 1.032e-01]\n",
      "Epoch 2: Loss = nan\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_msa.weight                          | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_msa.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "Epoch 3: Loss = nan\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_msa.weight                          | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_msa.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\n",
      "============================================================\n",
      "7. 测试 MSAColGlobalAttention\n",
      "============================================================\n",
      "query_flash shape: torch.Size([16, 4, 1, 4]) key_flash shape: torch.Size([16, 1, 1, 4]) value_flash shape: torch.Size([16, 1, 1, 4])\n",
      "Epoch 1: Loss = 1.271822\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_msa.weight                          | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_msa.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (4, 16)              |        nan |    64 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (4, 16)              |        nan |    64 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "to_out.bias                              | (16,)                |  2.897e-01 |     0 |     0 |   0.0% | [-1.315e-01, 1.256e-01]\n",
      "query_flash shape: torch.Size([16, 4, 1, 4]) key_flash shape: torch.Size([16, 1, 1, 4]) value_flash shape: torch.Size([16, 1, 1, 4])\n",
      "Epoch 2: Loss = nan\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_msa.weight                          | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_msa.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (4, 16)              |        nan |    64 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (4, 16)              |        nan |    64 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "query_flash shape: torch.Size([16, 4, 1, 4]) key_flash shape: torch.Size([16, 1, 1, 4]) value_flash shape: torch.Size([16, 1, 1, 4])\n",
      "Epoch 3: Loss = nan\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[91mnorm_msa.weight                          | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mnorm_msa.bias                            | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_q.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_k.weight                              | (4, 16)              |        nan |    64 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_v.weight                              | (4, 16)              |        nan |    64 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.weight                              | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_g.bias                                | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.weight                            | (16, 16)             |        nan |   256 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\u001b[91mto_out.bias                              | (16,)                |        nan |    16 |     0 |   0.0% | [nan, nan]\u001b[0m\n",
      "\n",
      "============================================================\n",
      "8. 测试 BiasedAxialAttention (行方向)\n",
      "============================================================\n",
      "pair shape: torch.Size([2, 8, 8, 24]) bias shape: torch.Size([2, 8, 8, 32])\n",
      "q_flash torch.Size([16, 4, 8, 4])\n",
      "k_flash torch.Size([16, 4, 8, 4])\n",
      "v_flash torch.Size([16, 4, 8, 4])\n",
      "total_bias torch.Size([16, 4, 8, 8])\n",
      "out_flash torch.Size([16, 4, 8, 4])\n",
      "out torch.Size([2, 8, 8, 16])\n",
      "gate torch.Size([2, 8, 8, 16])\n",
      "Epoch 1: Loss = 1.098584\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (24, 16)             |  1.597e-01 |     0 |     0 |   0.0% | [-2.650e-02, 2.939e-02]\n",
      "to_v.weight                              | (16, 24)             |  1.037e-01 |     0 |     0 |  25.0% | [-1.947e-02, 2.112e-02]\n",
      "to_out.bias                              | (24,)                |  9.919e-02 |     0 |     0 |   0.0% | [-3.274e-02, 4.276e-02]\n",
      "to_k.weight                              | (16, 24)             |  8.146e-02 |     0 |     0 |  25.0% | [-2.429e-02, 1.572e-02]\n",
      "norm_pair.weight                         | (24,)                |  6.244e-02 |     0 |     0 |   0.0% | [-5.327e-03, 2.486e-02]\n",
      "to_q.weight                              | (16, 24)             |  5.563e-02 |     0 |     0 |  25.0% | [-9.607e-03, 9.496e-03]\n",
      "norm_pair.bias                           | (24,)                |  4.368e-02 |     0 |     0 |   0.0% | [-1.678e-02, 1.675e-02]\n",
      "to_g.weight                              | (16, 24)             |  3.228e-02 |     0 |     0 |   0.0% | [-6.662e-03, 4.714e-03]\n",
      "to_g.bias                                | (16,)                |  1.151e-02 |     0 |     0 |   0.0% | [-1.430e-03, 5.261e-03]\n",
      "to_b.weight                              | (4, 32)              |  1.446e-08 |     0 |     0 |  25.0% | [-4.441e-09, 3.887e-09]\n",
      "norm_bias.bias                           | (32,)                |  1.951e-09 |     0 |     0 |   0.0% | [-6.984e-10, 7.022e-10]\n",
      "norm_bias.weight                         | (32,)                |  1.859e-09 |     0 |     0 |   0.0% | [-5.445e-10, 6.717e-10]\n",
      "pair shape: torch.Size([2, 8, 8, 24]) bias shape: torch.Size([2, 8, 8, 32])\n",
      "q_flash torch.Size([16, 4, 8, 4])\n",
      "k_flash torch.Size([16, 4, 8, 4])\n",
      "v_flash torch.Size([16, 4, 8, 4])\n",
      "total_bias torch.Size([16, 4, 8, 8])\n",
      "out_flash torch.Size([16, 4, 8, 4])\n",
      "out torch.Size([2, 8, 8, 16])\n",
      "gate torch.Size([2, 8, 8, 16])\n",
      "Epoch 2: Loss = 1.091801\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (24, 16)             |  1.564e-01 |     0 |     0 |   0.0% | [-2.611e-02, 2.880e-02]\n",
      "to_v.weight                              | (16, 24)             |  1.008e-01 |     0 |     0 |  25.0% | [-1.900e-02, 2.060e-02]\n",
      "to_out.bias                              | (24,)                |  9.869e-02 |     0 |     0 |   0.0% | [-3.265e-02, 4.251e-02]\n",
      "to_k.weight                              | (16, 24)             |  7.997e-02 |     0 |     0 |  25.0% | [-2.387e-02, 1.551e-02]\n",
      "norm_pair.weight                         | (24,)                |  5.941e-02 |     0 |     0 |   0.0% | [-5.625e-03, 2.386e-02]\n",
      "to_q.weight                              | (16, 24)             |  5.447e-02 |     0 |     0 |  25.0% | [-9.487e-03, 9.491e-03]\n",
      "norm_pair.bias                           | (24,)                |  4.268e-02 |     0 |     0 |   0.0% | [-1.636e-02, 1.616e-02]\n",
      "to_g.weight                              | (16, 24)             |  3.149e-02 |     0 |     0 |   0.0% | [-6.441e-03, 4.519e-03]\n",
      "to_g.bias                                | (16,)                |  1.104e-02 |     0 |     0 |   0.0% | [-1.433e-03, 5.047e-03]\n",
      "to_b.weight                              | (4, 32)              |  1.088e-08 |     0 |     0 |  25.0% | [-2.588e-09, 2.774e-09]\n",
      "norm_bias.bias                           | (32,)                |  1.917e-09 |     0 |     0 |   0.0% | [-6.989e-10, 5.257e-10]\n",
      "norm_bias.weight                         | (32,)                |  1.441e-09 |     0 |     0 |   0.0% | [-6.638e-10, 5.311e-10]\n",
      "pair shape: torch.Size([2, 8, 8, 24]) bias shape: torch.Size([2, 8, 8, 32])\n",
      "q_flash torch.Size([16, 4, 8, 4])\n",
      "k_flash torch.Size([16, 4, 8, 4])\n",
      "v_flash torch.Size([16, 4, 8, 4])\n",
      "total_bias torch.Size([16, 4, 8, 8])\n",
      "out_flash torch.Size([16, 4, 8, 4])\n",
      "out torch.Size([2, 8, 8, 16])\n",
      "gate torch.Size([2, 8, 8, 16])\n",
      "Epoch 3: Loss = 1.085178\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (24, 16)             |  1.532e-01 |     0 |     0 |   0.0% | [-2.573e-02, 2.822e-02]\n",
      "to_out.bias                              | (24,)                |  9.818e-02 |     0 |     0 |   0.0% | [-3.257e-02, 4.227e-02]\n",
      "to_v.weight                              | (16, 24)             |  9.793e-02 |     0 |     0 |  25.0% | [-1.853e-02, 2.010e-02]\n",
      "to_k.weight                              | (16, 24)             |  7.855e-02 |     0 |     0 |  25.0% | [-2.346e-02, 1.530e-02]\n",
      "norm_pair.weight                         | (24,)                |  5.649e-02 |     0 |     0 |   0.0% | [-5.920e-03, 2.289e-02]\n",
      "to_q.weight                              | (16, 24)             |  5.333e-02 |     0 |     0 |  25.0% | [-9.350e-03, 9.464e-03]\n",
      "norm_pair.bias                           | (24,)                |  4.171e-02 |     0 |     0 |   0.0% | [-1.597e-02, 1.558e-02]\n",
      "to_g.weight                              | (16, 24)             |  3.072e-02 |     0 |     0 |   0.0% | [-6.229e-03, 4.331e-03]\n",
      "to_g.bias                                | (16,)                |  1.060e-02 |     0 |     0 |   0.0% | [-1.435e-03, 4.840e-03]\n",
      "to_b.weight                              | (4, 32)              |  2.098e-08 |     0 |     0 |  25.0% | [-9.212e-09, 5.262e-09]\n",
      "norm_bias.bias                           | (32,)                |  4.878e-09 |     0 |     0 |   0.0% | [-1.908e-09, 1.668e-09]\n",
      "norm_bias.weight                         | (32,)                |  2.733e-09 |     0 |     0 |   0.0% | [-9.905e-10, 1.013e-09]\n",
      "\n",
      "============================================================\n",
      "9. 测试 BiasedAxialAttention (列方向)\n",
      "============================================================\n",
      "pair shape: torch.Size([2, 8, 8, 24]) bias shape: torch.Size([2, 8, 8, 32])\n",
      "q_flash torch.Size([16, 4, 8, 4])\n",
      "k_flash torch.Size([16, 4, 8, 4])\n",
      "v_flash torch.Size([16, 4, 8, 4])\n",
      "total_bias torch.Size([16, 4, 8, 8])\n",
      "out_flash torch.Size([16, 4, 8, 4])\n",
      "out torch.Size([2, 8, 8, 16])\n",
      "gate torch.Size([2, 8, 8, 16])\n",
      "Epoch 1: Loss = 1.033499\n",
      "\n",
      "=== Epoch 1 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (24, 16)             |  1.138e-01 |     0 |     0 |   0.0% | [-1.753e-02, 1.739e-02]\n",
      "to_v.weight                              | (16, 24)             |  8.784e-02 |     0 |     0 |  25.0% | [-1.476e-02, 1.799e-02]\n",
      "to_out.bias                              | (24,)                |  8.126e-02 |     0 |     0 |   0.0% | [-2.924e-02, 3.204e-02]\n",
      "to_q.weight                              | (16, 24)             |  4.993e-02 |     0 |     0 |  25.0% | [-9.968e-03, 9.349e-03]\n",
      "to_k.weight                              | (16, 24)             |  4.678e-02 |     0 |     0 |  25.0% | [-7.478e-03, 1.394e-02]\n",
      "norm_pair.bias                           | (24,)                |  4.412e-02 |     0 |     0 |   0.0% | [-1.964e-02, 1.798e-02]\n",
      "norm_pair.weight                         | (24,)                |  2.951e-02 |     0 |     0 |   0.0% | [-7.676e-03, 1.043e-02]\n",
      "to_g.weight                              | (16, 24)             |  2.117e-02 |     0 |     0 |   0.0% | [-3.928e-03, 2.992e-03]\n",
      "to_g.bias                                | (16,)                |  6.607e-03 |     0 |     0 |   0.0% | [-7.438e-04, 3.548e-03]\n",
      "to_b.weight                              | (4, 32)              |  6.443e-09 |     0 |     0 |  25.0% | [-2.099e-09, 1.623e-09]\n",
      "norm_bias.weight                         | (32,)                |  8.702e-10 |     0 |     0 |   0.0% | [-3.697e-10, 3.163e-10]\n",
      "norm_bias.bias                           | (32,)                |  3.328e-10 |     0 |     0 |   0.0% | [-1.066e-10, 1.313e-10]\n",
      "pair shape: torch.Size([2, 8, 8, 24]) bias shape: torch.Size([2, 8, 8, 32])\n",
      "q_flash torch.Size([16, 4, 8, 4])\n",
      "k_flash torch.Size([16, 4, 8, 4])\n",
      "v_flash torch.Size([16, 4, 8, 4])\n",
      "total_bias torch.Size([16, 4, 8, 8])\n",
      "out_flash torch.Size([16, 4, 8, 4])\n",
      "out torch.Size([2, 8, 8, 16])\n",
      "gate torch.Size([2, 8, 8, 16])\n",
      "Epoch 2: Loss = 1.028390\n",
      "\n",
      "=== Epoch 2 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (24, 16)             |  1.122e-01 |     0 |     0 |   0.0% | [-1.730e-02, 1.698e-02]\n",
      "to_v.weight                              | (16, 24)             |  8.609e-02 |     0 |     0 |  25.0% | [-1.437e-02, 1.764e-02]\n",
      "to_out.bias                              | (24,)                |  8.074e-02 |     0 |     0 |   0.0% | [-2.903e-02, 3.204e-02]\n",
      "to_q.weight                              | (16, 24)             |  4.850e-02 |     0 |     0 |  25.0% | [-9.717e-03, 9.147e-03]\n",
      "to_k.weight                              | (16, 24)             |  4.578e-02 |     0 |     0 |  25.0% | [-7.423e-03, 1.357e-02]\n",
      "norm_pair.bias                           | (24,)                |  4.291e-02 |     0 |     0 |   0.0% | [-1.918e-02, 1.735e-02]\n",
      "norm_pair.weight                         | (24,)                |  2.789e-02 |     0 |     0 |   0.0% | [-7.575e-03, 9.953e-03]\n",
      "to_g.weight                              | (16, 24)             |  2.082e-02 |     0 |     0 |   0.0% | [-3.874e-03, 2.950e-03]\n",
      "to_g.bias                                | (16,)                |  6.357e-03 |     0 |     0 |   0.0% | [-7.968e-04, 3.429e-03]\n",
      "to_b.weight                              | (4, 32)              |  6.843e-09 |     0 |     0 |  25.0% | [-1.977e-09, 2.084e-09]\n",
      "norm_bias.weight                         | (32,)                |  1.043e-09 |     0 |     0 |   0.0% | [-6.079e-10, 3.610e-10]\n",
      "norm_bias.bias                           | (32,)                |  1.020e-09 |     0 |     0 |   0.0% | [-4.355e-10, 4.943e-10]\n",
      "pair shape: torch.Size([2, 8, 8, 24]) bias shape: torch.Size([2, 8, 8, 32])\n",
      "q_flash torch.Size([16, 4, 8, 4])\n",
      "k_flash torch.Size([16, 4, 8, 4])\n",
      "v_flash torch.Size([16, 4, 8, 4])\n",
      "total_bias torch.Size([16, 4, 8, 8])\n",
      "out_flash torch.Size([16, 4, 8, 4])\n",
      "out torch.Size([2, 8, 8, 16])\n",
      "gate torch.Size([2, 8, 8, 16])\n",
      "Epoch 3: Loss = 1.023383\n",
      "\n",
      "=== Epoch 3 梯度信息 ===\n",
      "参数名称                                     | 形状                   |         范数 |   NaN |   Inf |    零值% | 范围\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "to_out.weight                            | (24, 16)             |  1.107e-01 |     0 |     0 |   0.0% | [-1.708e-02, 1.657e-02]\n",
      "to_v.weight                              | (16, 24)             |  8.441e-02 |     0 |     0 |  25.0% | [-1.399e-02, 1.730e-02]\n",
      "to_out.bias                              | (24,)                |  8.022e-02 |     0 |     0 |   0.0% | [-2.883e-02, 3.203e-02]\n",
      "to_q.weight                              | (16, 24)             |  4.711e-02 |     0 |     0 |  25.0% | [-9.462e-03, 8.936e-03]\n",
      "to_k.weight                              | (16, 24)             |  4.482e-02 |     0 |     0 |  25.0% | [-7.366e-03, 1.320e-02]\n",
      "norm_pair.bias                           | (24,)                |  4.175e-02 |     0 |     0 |   0.0% | [-1.874e-02, 1.673e-02]\n",
      "norm_pair.weight                         | (24,)                |  2.639e-02 |     0 |     0 |   0.0% | [-7.470e-03, 9.494e-03]\n",
      "to_g.weight                              | (16, 24)             |  2.049e-02 |     0 |     0 |   0.0% | [-3.819e-03, 2.909e-03]\n",
      "to_g.bias                                | (16,)                |  6.119e-03 |     0 |     0 |   0.0% | [-8.485e-04, 3.311e-03]\n",
      "to_b.weight                              | (4, 32)              |  5.635e-09 |     0 |     0 |  25.0% | [-1.315e-09, 1.766e-09]\n",
      "norm_bias.bias                           | (32,)                |  8.649e-10 |     0 |     0 |   0.0% | [-3.110e-10, 2.682e-10]\n",
      "norm_bias.weight                         | (32,)                |  7.787e-10 |     0 |     0 |   0.0% | [-2.734e-10, 3.601e-10]\n",
      "\n",
      "====================================================================================================\n",
      "所有 Attention 模块测试完成！\n",
      "====================================================================================================\n",
      "\n",
      "测试总结:\n",
      "- 批次大小: 2\n",
      "- 序列长度: 8\n",
      "- MSA序列数: 1\n",
      "- 模型维度: 16\n",
      "- 对维度: 24\n",
      "- 偏置维度: 32\n",
      "- 注意力头数: 4\n",
      "- 隐藏维度: 4\n",
      "- Padding策略: 无效位置设置为0\n",
      "- 有效位置数量: Batch 0: 5, Batch 1: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from opt_einsum import contract as einsum\n",
    "from rfdiffusion.util_module import init_lecun_normal\n",
    "from mytest.Attention_module import *\n",
    "\n",
    "def print_all_gradients(model, epoch=None):\n",
    "    \"\"\"打印模型中所有可训练参数的梯度信息\"\"\"\n",
    "    if epoch is not None:\n",
    "        print(f\"\\n=== Epoch {epoch} 梯度信息 ===\")\n",
    "    else:\n",
    "        print(f\"\\n=== 当前梯度信息 ===\")\n",
    "    \n",
    "    # 收集所有梯度信息\n",
    "    grad_data = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad = param.grad.data\n",
    "            grad_info = {\n",
    "                \"name\": name,\n",
    "                \"shape\": tuple(grad.shape),\n",
    "                \"dtype\": str(grad.dtype),\n",
    "                \"norm\": grad.norm().item(),\n",
    "                \"min\": grad.min().item(),\n",
    "                \"max\": grad.max().item(),\n",
    "                \"mean\": grad.mean().item(),\n",
    "                \"nan\": torch.isnan(grad).sum().item(),\n",
    "                \"inf\": torch.isinf(grad).sum().item(),\n",
    "                \"zero\": (grad == 0).sum().item()\n",
    "            }\n",
    "            grad_data.append(grad_info)\n",
    "    \n",
    "    # 按梯度范数排序（从大到小）\n",
    "    grad_data.sort(key=lambda x: x[\"norm\"], reverse=True)\n",
    "    \n",
    "    # 打印表格头\n",
    "    print(f\"{'参数名称':<40} | {'形状':<20} | {'范数':>10} | {'NaN':>5} | {'Inf':>5} | {'零值%':>6} | {'范围'}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # 打印每个参数的梯度信息\n",
    "    for info in grad_data:\n",
    "        total_elements = 1\n",
    "        for dim in info['shape']:\n",
    "            total_elements *= dim\n",
    "        zero_percent = info[\"zero\"] / total_elements * 100 if total_elements > 0 else 0\n",
    "        range_str = f\"[{info['min']:.3e}, {info['max']:.3e}]\"\n",
    "        \n",
    "        # 高亮异常梯度\n",
    "        if info[\"nan\"] > 0 or info[\"inf\"] > 0:\n",
    "            highlight = \"\\033[91m\"  # 红色\n",
    "            reset = \"\\033[0m\"\n",
    "        else:\n",
    "            highlight = reset = \"\"\n",
    "        \n",
    "        print(f\"{highlight}{info['name']:<40} | {str(info['shape']):<20} | \"\n",
    "            f\"{info['norm']:>10.3e} | \"\n",
    "            f\"{info['nan']:>5} | \"\n",
    "            f\"{info['inf']:>5} | \"\n",
    "            f\"{zero_percent:>5.1f}% | \"\n",
    "            f\"{range_str}{reset}\")\n",
    "\n",
    "def test_all_attention_modules_with_padding():\n",
    "    \"\"\"测试所有注意力模块，确保pad为0\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 设置基本参数\n",
    "    B, L = 2, 8\n",
    "    N = 1  # MSA序列数\n",
    "    d_model, d_pair, d_bias = 16,24,32\n",
    "    n_head, d_hidden = 4, 4\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"测试所有 Attention 模块 - 使用 padding=0\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # 创建mask - pad位置为0，有效位置为1\n",
    "    mask = torch.ones(B, L, dtype=torch.bool)\n",
    "    mask[0, 5:] = 0  # 第一个batch后3位为pad\n",
    "    mask[1, :3] = 0  # 第二个batch前3位为pad\n",
    "    mask[1, 6:] = 0  # 第二个batch后2位为pad\n",
    "    pad = float('nan')  # 使用NaN表示pad位置\n",
    "    print(f\"Mask shape: {mask.shape}\")\n",
    "    print(f\"Valid positions - Batch 0: {mask[0].sum()}, Batch 1: {mask[1].sum()}\")\n",
    "\n",
    "    # 1. 测试 FeedForwardLayer\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"1. 测试 FeedForwardLayer\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    data = torch.randn(B, L, d_model)\n",
    "    # 在pad位置设置为0而不是NaN\n",
    "    data[~mask] = pad\n",
    "    \n",
    "    ffn = FeedForwardLayer(d_model, 4)\n",
    "    optimizer = torch.optim.Adam(ffn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(data)\n",
    "    target[~mask] = 0.0\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = ffn(data)\n",
    "        # 只在有效位置计算loss\n",
    "        loss = F.mse_loss(out[mask], target[mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(ffn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 2. 测试 Attention\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"2. 测试 Attention\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    data = torch.randn(B, L, d_model)\n",
    "    data[~mask] = pad\n",
    "    \n",
    "    attn = Attention(d_model, d_model, n_head, d_hidden, d_model)\n",
    "    optimizer = torch.optim.Adam(attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(data)\n",
    "    target[~mask] = 0.0\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = attn(data, data, data, mask=mask)\n",
    "        print(out)\n",
    "        loss = F.mse_loss(out[mask], target[mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 3. 测试 AttentionWithBias\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"3. 测试 AttentionWithBias\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    data = torch.randn(B, L, d_model)\n",
    "    bias = torch.randn(B, L, L, d_bias)\n",
    "    data[~mask] = pad\n",
    "    # 修正bias的pad处理 - 需要正确的维度扩展\n",
    "    bias_mask = mask.unsqueeze(1).unsqueeze(-1) & mask.unsqueeze(2).unsqueeze(-1)  # [B, L, L, 1]\n",
    "    bias_mask = bias_mask.expand_as(bias)  # [B, L, L, d_bias]\n",
    "    bias[~bias_mask] = pad\n",
    "    \n",
    "    attn_bias = AttentionWithBias(d_model, d_bias, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(attn_bias.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(data)\n",
    "    target[~mask] = pad\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = attn_bias(data, bias, mask=mask)\n",
    "        loss = F.mse_loss(out[mask], target[mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(attn_bias, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 4. 测试 SequenceWeight\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"4. 测试 SequenceWeight\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    msa = torch.randn(B, N, L, d_model)\n",
    "    # 在MSA的pad位置设置为0\n",
    "    msa_mask = mask.unsqueeze(1).expand(B, N, L)  # [B, N, L]\n",
    "    msa[~msa_mask] = pad\n",
    "    \n",
    "    seq_weight = SequenceWeight(d_model, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(seq_weight.parameters(), lr=0.001)\n",
    "    target = torch.randn(B, N, L, n_head, 1)\n",
    "    target_mask = mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1).expand_as(target)\n",
    "    target[~target_mask] = pad\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = seq_weight(msa, mask=mask)\n",
    "        loss = F.mse_loss(out[target_mask], target[target_mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(seq_weight, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 5. 测试 MSARowAttentionWithBias\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"5. 测试 MSARowAttentionWithBias\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    msa = torch.randn(B, N, L, d_model)\n",
    "    pair = torch.randn(B, L, L, d_pair)\n",
    "    msa[~msa_mask] = 0.0\n",
    "    # pair的pad处理 - [B, L, L, d_pair]\n",
    "    pair_mask = mask.unsqueeze(1) & mask.unsqueeze(2)  # [B, L, L]\n",
    "    pair[~pair_mask.unsqueeze(-1).expand_as(pair)] = 0.0\n",
    "    \n",
    "    msa_row_attn = MSARowAttentionWithBias(d_model, d_pair, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(msa_row_attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(msa)\n",
    "    target[~msa_mask.unsqueeze(-1).expand_as(target)] = 0.0\n",
    "    msa.requires_grad_(True)  # 确保MSA的梯度被计算\n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = msa_row_attn(msa, pair, mask=mask)\n",
    "        print(out.shape)\n",
    "        target_mask_expanded = msa_mask.unsqueeze(-1).expand_as(out)\n",
    "        loss = F.mse_loss(out[target_mask_expanded], target[target_mask_expanded])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print(msa.grad)\n",
    "        print_all_gradients(msa_row_attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 6. 测试 MSAColAttention\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"6. 测试 MSAColAttention\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    msa = torch.randn(B, N, L, d_model)\n",
    "    msa[~msa_mask] = pad\n",
    "    \n",
    "    msa_col_attn = MSAColAttention(d_model, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(msa_col_attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(msa)\n",
    "    target[~msa_mask.unsqueeze(-1).expand_as(target)] = pad\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = msa_col_attn(msa, mask=mask)\n",
    "        target_mask_expanded = msa_mask.unsqueeze(-1).expand_as(out)\n",
    "        loss = F.mse_loss(out[target_mask_expanded], target[target_mask_expanded])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(msa_col_attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 7. 测试 MSAColGlobalAttention\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"7. 测试 MSAColGlobalAttention\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    msa = torch.randn(B, N, L, d_model)\n",
    "    msa[~msa_mask] = pad\n",
    "    \n",
    "    msa_global_attn = MSAColGlobalAttention(d_model, n_head, d_hidden)\n",
    "    optimizer = torch.optim.Adam(msa_global_attn.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(msa)\n",
    "    target[~msa_mask.unsqueeze(-1).expand_as(target)] = pad\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = msa_global_attn(msa, mask=mask)\n",
    "        target_mask_expanded = msa_mask.unsqueeze(-1).expand_as(out)\n",
    "        loss = F.mse_loss(out[target_mask_expanded], target[target_mask_expanded])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(msa_global_attn, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 8. 测试 BiasedAxialAttention (行方向)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"8. 测试 BiasedAxialAttention (行方向)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pair = torch.randn(B, L, L, d_pair)\n",
    "    bias = torch.randn(B, L, L, d_bias)\n",
    "    pair[~pair_mask.unsqueeze(-1).expand_as(pair)] = pad\n",
    "    bias[~pair_mask.unsqueeze(-1).expand_as(bias)] = pad\n",
    "    \n",
    "    axial_attn_row = BiasedAxialAttention(d_pair, d_bias, n_head, d_hidden, is_row=True)\n",
    "    optimizer = torch.optim.Adam(axial_attn_row.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(pair)\n",
    "    target[~pair_mask.unsqueeze(-1).expand_as(target)] = pad\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = axial_attn_row(pair, bias, mask=mask)\n",
    "        loss_mask = pair_mask.unsqueeze(-1).expand_as(out)\n",
    "        loss = F.mse_loss(out[loss_mask], target[loss_mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(axial_attn_row, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    # 9. 测试 BiasedAxialAttention (列方向)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"9. 测试 BiasedAxialAttention (列方向)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pair = torch.randn(B, L, L, d_pair)\n",
    "    bias = torch.randn(B, L, L, d_bias)\n",
    "    pair[~pair_mask.unsqueeze(-1).expand_as(pair)] = pad\n",
    "    bias[~pair_mask.unsqueeze(-1).expand_as(bias)] = pad\n",
    "    \n",
    "    axial_attn_col = BiasedAxialAttention(d_pair, d_bias, n_head, d_hidden, is_row=False)\n",
    "    optimizer = torch.optim.Adam(axial_attn_col.parameters(), lr=0.001)\n",
    "    target = torch.randn_like(pair)\n",
    "    target[~pair_mask.unsqueeze(-1).expand_as(target)] = pad\n",
    "    \n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        out = axial_attn_col(pair, bias, mask=mask)\n",
    "        loss_mask = pair_mask.unsqueeze(-1).expand_as(out)\n",
    "        loss = F.mse_loss(out[loss_mask], target[loss_mask])\n",
    "        print(f\"Epoch {i+1}: Loss = {loss.item():.6f}\")\n",
    "        loss.backward()\n",
    "        print_all_gradients(axial_attn_col, epoch=i+1)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"所有 Attention 模块测试完成！\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # 测试总结\n",
    "    print(\"\\n测试总结:\")\n",
    "    print(f\"- 批次大小: {B}\")\n",
    "    print(f\"- 序列长度: {L}\")\n",
    "    print(f\"- MSA序列数: {N}\")\n",
    "    print(f\"- 模型维度: {d_model}\")\n",
    "    print(f\"- 对维度: {d_pair}\")\n",
    "    print(f\"- 偏置维度: {d_bias}\")\n",
    "    print(f\"- 注意力头数: {n_head}\")\n",
    "    print(f\"- 隐藏维度: {d_hidden}\")\n",
    "    print(f\"- Padding策略: 无效位置设置为0\")\n",
    "    print(f\"- 有效位置数量: Batch 0: {mask[0].sum()}, Batch 1: {mask[1].sum()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_all_attention_modules_with_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346bfc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "参数名称: weight, 形状: torch.Size([64]),  数据类型: torch.float32\n",
      "参数名称: bias, 形状: torch.Size([64]),  数据类型: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch.nn  as nn \n",
    " \n",
    "# 创建 LayerNorm 层 \n",
    "layer = nn.LayerNorm(64)\n",
    "print(layer)\n",
    "# 打印所有参数及其名称 \n",
    "for name, param in layer.named_parameters(): \n",
    "    print(f\"参数名称: {name}, 形状: {param.shape},  数据类型: {param.dtype}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE3nv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
