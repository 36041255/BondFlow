{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce39621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import MMCIFParser\n",
    "from Bio.PDB.PDBExceptions import PDBConstructionWarning\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27446d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n",
    "\n",
    "def extract_link_info_from_mmcif(cif_file):\n",
    "    \"\"\"\n",
    "    使用 MMCIF2Dict 从 mmCIF 文件中提取 LINK (struct_conn) 信息。\n",
    "\n",
    "    Args:\n",
    "        cif_file (str): mmCIF 文件的路径。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含连接信息的字典列表。\n",
    "    \"\"\"\n",
    "    mmcif_dict = MMCIF2Dict(cif_file)\n",
    "    \n",
    "    link_info = []\n",
    "    \n",
    "    # 检查文件中是否存在 _struct_conn 信息\n",
    "    if '_struct_conn.id' not in mmcif_dict:\n",
    "        print(\"文件中未找到 LINK (struct_conn) 信息。\")\n",
    "        return link_info\n",
    "\n",
    "    num_connections = len(mmcif_dict['_struct_conn.id'])\n",
    "    \n",
    "    for i in range(num_connections):\n",
    "        conn_info = {\n",
    "            'id': mmcif_dict['_struct_conn.id'][i],\n",
    "            'type': mmcif_dict.get('_struct_conn.conn_type_id', ['n/a'])[i],\n",
    "            'partner1': {\n",
    "                'chain': mmcif_dict.get('_struct_conn.ptnr1_label_asym_id', ['n/a'])[i],\n",
    "                'residue_name': mmcif_dict.get('_struct_conn.ptnr1_label_comp_id', ['n/a'])[i],\n",
    "                'residue_seq': mmcif_dict.get('_struct_conn.ptnr1_label_seq_id', ['n/a'])[i],\n",
    "                'atom': mmcif_dict.get('_struct_conn.ptnr1_label_atom_id', ['n/a'])[i]\n",
    "            },\n",
    "            'partner2': {\n",
    "                'chain': mmcif_dict.get('_struct_conn.ptnr2_label_asym_id', ['n/a'])[i],\n",
    "                'residue_name': mmcif_dict.get('_struct_conn.ptnr2_label_comp_id', ['n/a'])[i],\n",
    "                'residue_seq': mmcif_dict.get('_struct_conn.ptnr2_label_seq_id', ['n/a'])[i],\n",
    "                'atom': mmcif_dict.get('_struct_conn.ptnr2_label_atom_id', ['n/a'])[i]\n",
    "            }\n",
    "        }\n",
    "        link_info.append(conn_info)\n",
    "        \n",
    "    return link_info\n",
    "\n",
    "# --- 示例使用 ---\n",
    "# 假设你有一个名为 \"1a2b.cif\" 的文件\n",
    "# 你可以从 RCSB PDB 数据库下载任何包含非标准连接的结构文件进行测试，例如 4HHB\n",
    "# from Bio.PDB import PDBList\n",
    "# pdbl = PDBList()\n",
    "# pdbl.retrieve_pdb_file('4HHB', pdir='.', file_format='mmCif')\n",
    "\n",
    "try:\n",
    "    cif_file_path = '3a55.cif' # 将文件名替换为你的文件\n",
    "    links = extract_link_info_from_mmcif(cif_file_path)\n",
    "\n",
    "    for link in links:\n",
    "        print(f\"ID: {link['id']}, 类型: {link['type']}\")\n",
    "        p1 = link['partner1']\n",
    "        p2 = link['partner2']\n",
    "        print(f\"  伙伴1: 链 {p1['chain']}, 残基 {p1['residue_name']} {p1['residue_seq']}, 原子 {p1['atom']}\")\n",
    "        print(f\"  伙伴2: 链 {p2['chain']}, 残基 {p2['residue_name']} {p2['residue_seq']}, 原子 {p2['atom']}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"请确保 CIF 文件存在于正确的路径。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bb148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将使用 104 个核心进行最终分析 (键长阈值: 5.0 Å)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析文件(含键长):   0%|          | 92/195048 [00:16<9:34:56,  5.65it/s] \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/WORK/.conda/envs/SE3nv/lib/python3.9/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 222\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/CIF_ALL_DATASET\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    220\u001b[0m log_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/link\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 222\u001b[0m \u001b[43mrun_analysis_and_generate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 135\u001b[0m, in \u001b[0;36mrun_analysis_and_generate_outputs\u001b[0;34m(directory, output_basename)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mnum_processes) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(cif_files), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m分析文件(含键长)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m--> 135\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m file_name, bonds \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(analyze_cif_with_length, cif_files):\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m bonds:\n\u001b[1;32m    137\u001b[0m                 all_results\u001b[38;5;241m.\u001b[39mappend((file_name, bonds))\n",
      "File \u001b[0;32m~/WORK/.conda/envs/SE3nv/lib/python3.9/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/WORK/.conda/envs/SE3nv/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 全局常量定义 ---\n",
    "STANDARD_AMINO_ACIDS = {\n",
    "    'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLU', 'GLN', 'GLY', 'HIS', 'ILE',\n",
    "    'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRW', 'TYR', 'VAL'\n",
    "}\n",
    "MAIN_CHAIN_ATOMS = {'N', 'CA', 'C', 'O', 'OXT'}\n",
    "BOND_LENGTH_THRESHOLD = 5.0\n",
    "\n",
    "def build_coordinate_lookup(mmcif_dict):\n",
    "    \"\"\"\n",
    "    预扫描mmcif字典，构建一个原子坐标的速查字典。\n",
    "    键: (链ID, 残基号, 插入码, 原子名) -> 值: (x, y, z)\n",
    "    \"\"\"\n",
    "    lookup = {}\n",
    "    keys = [\n",
    "        '_atom_site.auth_asym_id', '_atom_site.auth_seq_id', \n",
    "        '_atom_site.pdbx_PDB_ins_code', '_atom_site.label_atom_id',\n",
    "        '_atom_site.Cartn_x', '_atom_site.Cartn_y', '_atom_site.Cartn_z'\n",
    "    ]\n",
    "    \n",
    "    # 检查所有必需的键是否存在\n",
    "    if not all(key in mmcif_dict for key in keys):\n",
    "        return None\n",
    "\n",
    "    chain_ids, res_nums, ins_codes, atom_names, xs, ys, zs = [mmcif_dict.get(k, []) for k in keys]\n",
    "    \n",
    "    for i in range(len(chain_ids)):\n",
    "        try:\n",
    "            # 使用更完整的键，包含插入码(ins_code)\n",
    "            key = (chain_ids[i], res_nums[i], ins_codes[i], atom_names[i])\n",
    "            coords = (float(xs[i]), float(ys[i]), float(zs[i]))\n",
    "            lookup[key] = coords\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "    return lookup\n",
    "\n",
    "def calculate_distance(coords1, coords2):\n",
    "    \"\"\"计算两个三维坐标点之间的欧氏距离。\"\"\"\n",
    "    return math.sqrt(sum([(a - b) ** 2 for a, b in zip(coords1, coords2)]))\n",
    "\n",
    "def analyze_cif_with_length(file_path):\n",
    "    \"\"\"最终版分析逻辑：计算键长并根据阈值过滤。\"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    found_bonds = []\n",
    "\n",
    "    try:\n",
    "        mmcif_dict = MMCIF2Dict(file_path)\n",
    "        coord_lookup = build_coordinate_lookup(mmcif_dict)\n",
    "        if coord_lookup is None:\n",
    "            return file_name, []\n",
    "            \n",
    "        conn_ids = mmcif_dict.get('_struct_conn.id', [])\n",
    "        if not conn_ids:\n",
    "            return file_name, []\n",
    "\n",
    "        conn_data = {k: mmcif_dict.get(k, []) for k in [\n",
    "            '_struct_conn.conn_type_id',\n",
    "            '_struct_conn.ptnr1_auth_asym_id', '_struct_conn.ptnr1_auth_comp_id', '_struct_conn.ptnr1_auth_seq_id', '_struct_conn.pdbx_ptnr1_PDB_ins_code', '_struct_conn.ptnr1_label_atom_id',\n",
    "            '_struct_conn.ptnr2_auth_asym_id', '_struct_conn.ptnr2_auth_comp_id', '_struct_conn.ptnr2_auth_seq_id', '_struct_conn.pdbx_ptnr2_PDB_ins_code', '_struct_conn.ptnr2_label_atom_id'\n",
    "        ]}\n",
    "\n",
    "        for i in range(len(conn_ids)):\n",
    "            key1 = (conn_data['_struct_conn.ptnr1_auth_asym_id'][i], conn_data['_struct_conn.ptnr1_auth_seq_id'][i], conn_data['_struct_conn.pdbx_ptnr1_PDB_ins_code'][i], conn_data['_struct_conn.ptnr1_label_atom_id'][i])\n",
    "            key2 = (conn_data['_struct_conn.ptnr2_auth_asym_id'][i], conn_data['_struct_conn.ptnr2_auth_seq_id'][i], conn_data['_struct_conn.pdbx_ptnr2_PDB_ins_code'][i], conn_data['_struct_conn.ptnr2_label_atom_id'][i])\n",
    "            \n",
    "            coords1 = coord_lookup.get(key1)\n",
    "            coords2 = coord_lookup.get(key2)\n",
    "\n",
    "            if not coords1 or not coords2:\n",
    "                continue\n",
    "\n",
    "            distance = calculate_distance(coords1, coords2)\n",
    "            if distance > BOND_LENGTH_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            bond_dict = {\n",
    "                \"p1_chain\": key1[0], \"p1_res_name\": conn_data['_struct_conn.ptnr1_auth_comp_id'][i], \"p1_res_num\": key1[1], \"p1_atom_name\": key1[3],\n",
    "                \"p2_chain\": key2[0], \"p2_res_name\": conn_data['_struct_conn.ptnr2_auth_comp_id'][i], \"p2_res_num\": key2[1], \"p2_atom_name\": key2[3],\n",
    "                \"distance\": round(distance, 2)\n",
    "            }\n",
    "            \n",
    "            conn_type = conn_data['_struct_conn.conn_type_id'][i]\n",
    "            bond_type = None\n",
    "\n",
    "            if conn_type == 'disulf':\n",
    "                bond_type = 'disulfide'\n",
    "                if bond_dict[\"p1_res_name\"] not in STANDARD_AMINO_ACIDS or bond_dict[\"p2_res_name\"] not in STANDARD_AMINO_ACIDS:\n",
    "                    continue\n",
    "            elif conn_type == 'covale':\n",
    "                if bond_dict[\"p1_res_name\"] not in STANDARD_AMINO_ACIDS or bond_dict[\"p2_res_name\"] not in STANDARD_AMINO_ACIDS:\n",
    "                    continue\n",
    "                is_p1_main = bond_dict[\"p1_atom_name\"] in MAIN_CHAIN_ATOMS\n",
    "                is_p2_main = bond_dict[\"p2_atom_name\"] in MAIN_CHAIN_ATOMS\n",
    "\n",
    "                # 只要不是纯主链连接，就视为异肽键\n",
    "                if not (is_p1_main and is_p2_main):\n",
    "                    bond_type = 'isopeptide'\n",
    "            \n",
    "            if bond_type:\n",
    "                bond_dict['bond_type'] = bond_type\n",
    "                found_bonds.append(bond_dict)\n",
    "    \n",
    "    except Exception:\n",
    "        return file_name, []\n",
    "    \n",
    "    return file_name, found_bonds\n",
    "\n",
    "\n",
    "def run_analysis_and_generate_outputs(directory, output_basename):\n",
    "    \"\"\"主函数：执行带键长分析的完整流程。\"\"\"\n",
    "    cif_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.cif')]\n",
    "    if not cif_files:\n",
    "        print(f\"目录 '{directory}' 中无 .cif 文件。\")\n",
    "        return\n",
    "    \n",
    "    report_txt_path = output_basename + \"_report.txt\"\n",
    "    data_csv_path = output_basename + \"_data.csv\"\n",
    "\n",
    "    num_processes = cpu_count()\n",
    "    print(f\"将使用 {num_processes} 个核心进行最终分析 (键长阈值: {BOND_LENGTH_THRESHOLD} Å)...\")\n",
    "\n",
    "    all_results = []\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        with tqdm(total=len(cif_files), desc=\"分析文件(含键长)\") as pbar:\n",
    "            for file_name, bonds in pool.imap_unordered(analyze_cif_with_length, cif_files):\n",
    "                if bonds:\n",
    "                    all_results.append((file_name, bonds))\n",
    "                pbar.update(1)\n",
    "\n",
    "    # --- 1. 写入机器可读的 .csv 文件 (新增 distance 列) ---\n",
    "    csv_fieldnames = [\n",
    "        'file_name', 'bond_type', 'distance',\n",
    "        'p1_chain', 'p1_res_name', 'p1_res_num', 'p1_atom_name',\n",
    "        'p2_chain', 'p2_res_name', 'p2_res_num', 'p2_atom_name'\n",
    "    ]\n",
    "    try:\n",
    "        with open(data_csv_path, 'w', newline='', encoding='utf-8') as f_csv:\n",
    "            writer = csv.DictWriter(f_csv, fieldnames=csv_fieldnames)\n",
    "            writer.writeheader()\n",
    "            for file_name, bonds in sorted(all_results):\n",
    "                for bond in bonds:\n",
    "                    row = bond.copy()\n",
    "                    row['file_name'] = file_name\n",
    "                    writer.writerow(row)\n",
    "        print(f\"\\n[成功] 机器可读数据已写入: {data_csv_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"\\n[错误] 无法写入CSV文件: {e}\")\n",
    "\n",
    "    # --- 2. 生成并写入人类可读的 .txt 报告 (完整版) ---\n",
    "    report_lines = []\n",
    "    total_files = len(cif_files)\n",
    "    \n",
    "    stats = {'isopeptide': set(), 'disulfide': set()}\n",
    "    bond_counts = {'isopeptide': 0, 'disulfide': 0}\n",
    "\n",
    "    for file_name, bonds in all_results:\n",
    "        for bond in bonds:\n",
    "            b_type = bond['bond_type']\n",
    "            if b_type in stats:\n",
    "                stats[b_type].add(file_name)\n",
    "                bond_counts[b_type] += 1\n",
    "            \n",
    "    report_lines.append(\"=\"*60)\n",
    "    report_lines.append(f\"--- 分析报告 (键长阈值: <={BOND_LENGTH_THRESHOLD} Å) ---\")\n",
    "    report_lines.append(f\"分析时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report_lines.append(f\"分析目录: {os.path.abspath(directory)}\")\n",
    "    report_lines.append(f\"总计分析文件数: {total_files}\")\n",
    "    report_lines.append(\"=\"*60)\n",
    "\n",
    "    report_lines.append(\"\\n--- 分析摘要 ---\")\n",
    "    report_lines.append(f\"异肽键: {bond_counts['isopeptide']} 个 (分布于 {len(stats['isopeptide'])} 个蛋白质)\")\n",
    "    report_lines.append(f\"二硫键: {bond_counts['disulfide']} 个 (分布于 {len(stats['disulfide'])} 个蛋白质)\")\n",
    "    \n",
    "    report_lines.append(\"\\n--- 详细信息 ---\")\n",
    "    if not all_results:\n",
    "        report_lines.append(\"未在任何文件中检测到符合条件的化学键。\")\n",
    "    else:\n",
    "        for file_name, bonds in sorted(all_results):\n",
    "            report_lines.append(f\"\\n[文件: {file_name}]\")\n",
    "            bonds_by_type = {'isopeptide': [], 'disulfide': []}\n",
    "            for bond in bonds:\n",
    "                if bond['bond_type'] in bonds_by_type:\n",
    "                    bonds_by_type[bond['bond_type']].append(bond)\n",
    "\n",
    "            for b_type, b_list in bonds_by_type.items():\n",
    "                if b_list:\n",
    "                    type_map = {'isopeptide': '异肽键', 'disulfide': '二硫键'}\n",
    "                    report_lines.append(f\"  - {type_map[b_type]} ({len(b_list)} 个):\")\n",
    "                    for i, bond in enumerate(b_list):\n",
    "                        p1 = f\"{bond['p1_chain']}:{bond['p1_res_name']}{bond['p1_res_num']}:{bond['p1_atom_name']}\"\n",
    "                        p2 = f\"{bond['p2_chain']}:{bond['p2_res_name']}{bond['p2_res_num']}:{bond['p2_atom_name']}\"\n",
    "                        report_lines.append(f\"    {i+1}. {p1} <--> {p2} (键长: {bond['distance']:.2f} Å)\")\n",
    "    \n",
    "    report_lines.append(\"\\n\" + \"=\"*60)\n",
    "\n",
    "    try:\n",
    "        with open(report_txt_path, 'w', encoding='utf-8') as f_txt:\n",
    "            f_txt.write(\"\\n\".join(report_lines))\n",
    "        print(f\"[成功] 人类可读报告已写入: {report_txt_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"[错误] 无法写入报告文件: {e}\")\n",
    "        \n",
    "    # --- 3. 在控制台打印报告预览 ---\n",
    "    print(\"\\n--- 控制台报告预览 ---\")\n",
    "    print(\"\\n\".join(report_lines))\n",
    "    print(\"\\n分析流程全部完成！\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dir = '/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/CIF_ALL_DATASET'\n",
    "    log_file = '/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/link'\n",
    "\n",
    "    run_analysis_and_generate_outputs(dir, log_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f1c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 每种氨基酸对的最常见原子配对方式 (已去重) ---\n",
      "\n",
      "氨基酸对: ALA - SER (总出现次数: 15)\n",
      "  -> 最常见的原子配对: ALA[C] - SER[OG] (出现 15 次, 平均距离: 1.41 Å)\n",
      "\n",
      "氨基酸对: ARG - ASP (总出现次数: 13)\n",
      "  -> 最常见的原子配对: ARG[N] - ASP[CG] (出现 4 次, 平均距离: 1.34 Å)\n",
      "\n",
      "氨基酸对: ARG - GLU (总出现次数: 24)\n",
      "  -> 最常见的原子配对: ARG[NH2] - GLU[OE1] (出现 6 次, 平均距离: 1.32 Å)\n",
      "\n",
      "氨基酸对: ASN - LYS (总出现次数: 131)\n",
      "  -> 最常见的原子配对: ASN[CG] - LYS[NZ] (出现 124 次, 平均距离: 1.37 Å)\n",
      "\n",
      "氨基酸对: ASN - SER (总出现次数: 16)\n",
      "  -> 最常见的原子配对: ASN[CG] - SER[OG] (出现 16 次, 平均距离: 1.44 Å)\n",
      "\n",
      "氨基酸对: ASP - CYS (总出现次数: 22)\n",
      "  -> 最常见的原子配对: ASP[CB] - CYS[SG] (出现 8 次, 平均距离: 1.79 Å)\n",
      "\n",
      "氨基酸对: ASP - CYS (总出现次数: 22)\n",
      "  -> 最常见的原子配对: ASP[C] - CYS[SG] (出现 5 次, 平均距离: 1.89 Å)\n",
      "\n",
      "氨基酸对: ASP - GLY (总出现次数: 12)\n",
      "  -> 最常见的原子配对: ASP[CG] - GLY[N] (出现 12 次, 平均距离: 1.33 Å)\n",
      "\n",
      "氨基酸对: ASP - LYS (总出现次数: 27)\n",
      "  -> 最常见的原子配对: ASP[CG] - LYS[NZ] (出现 17 次, 平均距离: 1.31 Å)\n",
      "\n",
      "氨基酸对: ASP - LYS (总出现次数: 27)\n",
      "  -> 最常见的原子配对: ASP[C] - LYS[NZ] (出现 4 次, 平均距离: 1.44 Å)\n",
      "\n",
      "氨基酸对: ASP - THR (总出现次数: 11)\n",
      "  -> 最常见的原子配对: ASP[CG] - THR[OG1] (出现 10 次, 平均距离: 1.39 Å)\n",
      "\n",
      "氨基酸对: CYS - CYS (总出现次数: 131735)\n",
      "  -> 最常见的原子配对: CYS[SG] - CYS[SG] (出现 131714 次, 平均距离: 2.06 Å)\n",
      "\n",
      "氨基酸对: CYS - GLU (总出现次数: 7)\n",
      "  -> 最常见的原子配对: CYS[SG] - GLU[CG] (出现 4 次, 平均距离: 1.91 Å)\n",
      "\n",
      "氨基酸对: CYS - GLY (总出现次数: 15)\n",
      "  -> 最常见的原子配对: CYS[SG] - GLY[C] (出现 11 次, 平均距离: 1.82 Å)\n",
      "\n",
      "氨基酸对: CYS - HIS (总出现次数: 10)\n",
      "  -> 最常见的原子配对: CYS[SG] - HIS[CE1] (出现 7 次, 平均距离: 1.80 Å)\n",
      "\n",
      "氨基酸对: CYS - TYR (总出现次数: 52)\n",
      "  -> 最常见的原子配对: CYS[SG] - TYR[CE2] (出现 33 次, 平均距离: 1.77 Å)\n",
      "\n",
      "氨基酸对: GLN - THR (总出现次数: 11)\n",
      "  -> 最常见的原子配对: GLN[CD] - THR[OG1] (出现 9 次, 平均距离: 1.33 Å)\n",
      "\n",
      "氨基酸对: GLN - TYR (总出现次数: 5)\n",
      "  -> 最常见的原子配对: GLN[CG] - TYR[CE1] (出现 4 次, 平均距离: 1.53 Å)\n",
      "\n",
      "氨基酸对: GLU - GLU (总出现次数: 10)\n",
      "  -> 最常见的原子配对: GLU[CD] - GLU[N] (出现 5 次, 平均距离: 1.38 Å)\n",
      "\n",
      "氨基酸对: GLU - GLY (总出现次数: 16)\n",
      "  -> 最常见的原子配对: GLU[CD] - GLY[N] (出现 16 次, 平均距离: 1.34 Å)\n",
      "\n",
      "氨基酸对: GLU - LYS (总出现次数: 41)\n",
      "  -> 最常见的原子配对: GLU[CD] - LYS[NZ] (出现 22 次, 平均距离: 1.49 Å)\n",
      "\n",
      "氨基酸对: GLY - LYS (总出现次数: 71)\n",
      "  -> 最常见的原子配对: GLY[C] - LYS[NZ] (出现 71 次, 平均距离: 1.34 Å)\n",
      "\n",
      "氨基酸对: GLY - SER (总出现次数: 6)\n",
      "  -> 最常见的原子配对: GLY[C] - SER[OG] (出现 6 次, 平均距离: 1.36 Å)\n",
      "\n",
      "氨基酸对: HIS - TYR (总出现次数: 58)\n",
      "  -> 最常见的原子配对: HIS[NE2] - TYR[CE2] (出现 38 次, 平均距离: 1.37 Å)\n",
      "\n",
      "氨基酸对: ILE - SER (总出现次数: 8)\n",
      "  -> 最常见的原子配对: ILE[C] - SER[OG] (出现 7 次, 平均距离: 1.41 Å)\n",
      "\n",
      "氨基酸对: MET - TYR (总出现次数: 61)\n",
      "  -> 最常见的原子配对: MET[SD] - TYR[CE2] (出现 46 次, 平均距离: 1.75 Å)\n",
      "\n",
      "氨基酸对: SER - TYR (总出现次数: 6)\n",
      "  -> 最常见的原子配对: SER[OG] - TYR[C] (出现 6 次, 平均距离: 1.55 Å)\n",
      "\n",
      "[成功] 分析结果已写入: /home/fit/lulei/WORK/xjt/Protein_design/RFdiffusion/mytest/config/inference/link.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "STANDARD_AMINO_ACIDS = {\n",
    "    'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLU', 'GLN', 'GLY', 'HIS', 'ILE',\n",
    "    'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRW', 'TYR', 'VAL'\n",
    "}\n",
    "result_file = '/home/fit/lulei/WORK/xjt/Protein_design/RFdiffusion/mytest/config/inference/link.csv'\n",
    "def analyze_atom_pairings(file_path):\n",
    "    \"\"\"\n",
    "    读取CSV文件，分析每种氨基酸对(residue pair)中，出现次数最多的原子对(atom pair)及其平均距离。\n",
    "    为消除冗余，每个文件内同类型的连接只统计一次。\n",
    "    \"\"\"\n",
    "\n",
    "    item_set = set()\n",
    "\n",
    "    with open(cleaned_chain_dir, 'r', encoding='utf-8') as f:\n",
    "        # 逐行读取文件内容\n",
    "        for line in f:\n",
    "            # 1. line.strip() - 去除行首和行尾的空白字符（如换行符 \\n）\n",
    "            # 2. .split('\\t') - 使用制表符 (\\t) 分割字符串，得到一个包含各列元素的列表\n",
    "            columns = line.strip().split('\\t')\n",
    "            \n",
    "            # 遍历分割后的所有元素\n",
    "            for item in columns:\n",
    "                # 如果元素非空，则将其添加到集合中\n",
    "                if item:\n",
    "                    item_set.add(item)\n",
    "\n",
    "    # 结构: { ('res1', 'res2'): {('atom1', 'atom2'): {'count': N, 'total_distance': X}} }\n",
    "    pair_data = defaultdict(lambda: defaultdict(lambda: {'count': 0, 'total_distance': 0.0}))\n",
    "    \n",
    "    # 用于跟踪每个文件内已处理的连接，避免重复计数\n",
    "    # 结构: { 'file_name': {('res1', 'res2', 'atom1', 'atom2')} }\n",
    "    processed_links_in_file = defaultdict(set)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                file_name = row['file_name']\n",
    "                num1, num2 = row['p1_res_num'], row['p2_res_num']\n",
    "                res1, res2 = row['p1_res_name'], row['p2_res_name']\n",
    "                atom1, atom2 = row['p1_atom_name'], row['p2_atom_name']\n",
    "                distance = float(row['distance'])\n",
    "                pdb_id = row['file_name'].split('.')[0]\n",
    "                chain1, chain2 = row['p1_chain'], row['p2_chain']\n",
    "                # if f\"{pdb_id}_{chain1}\" not in item_set and f\"{pdb_id}_{chain2}\" not in item_set:\n",
    "                #     continue\n",
    "                if (res1,num1,atom1) == (res2,num2,atom2):\n",
    "                    continue\n",
    "                if res1 not in STANDARD_AMINO_ACIDS or res2 not in STANDARD_AMINO_ACIDS:\n",
    "                    continue\n",
    "                \n",
    "                # 创建规范的氨基酸对和原子对（排序以保证唯一性）\n",
    "                res_pair_sorted = tuple(sorted((res1, res2)))\n",
    "                if res1 == res_pair_sorted[0] and res2 == res_pair_sorted[1]:\n",
    "                    atom_pair_sorted = (atom1, atom2)\n",
    "                    num_pair_sorted = (num1, num2)\n",
    "                else:\n",
    "                    atom_pair_sorted = (atom2, atom1)\n",
    "                    num_pair_sorted = (num2, num1)\n",
    "                \n",
    "                # 创建一个唯一的连接标识符\n",
    "                link_signature = (num_pair_sorted[0],num_pair_sorted[1],res_pair_sorted[0], res_pair_sorted[1], atom_pair_sorted[0], atom_pair_sorted[1])\n",
    "\n",
    "                # 如果此连接类型已在该文件中处理过，则跳过\n",
    "                if link_signature in processed_links_in_file[file_name]:\n",
    "                    continue\n",
    "                \n",
    "                # 标记此连接类型为已处理\n",
    "                processed_links_in_file[file_name].add(link_signature)\n",
    "\n",
    "                pair_data[res_pair_sorted][atom_pair_sorted]['count'] += 1\n",
    "                pair_data[res_pair_sorted][atom_pair_sorted]['total_distance'] += distance\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 '{file_path}'\")\n",
    "        return\n",
    "    except KeyError as e:\n",
    "        print(f\"错误: CSV文件中缺少必需的列: {e}\")\n",
    "        return\n",
    "    except ValueError:\n",
    "        print(f\"错误: CSV文件中的 'distance' 列包含非数值。\")\n",
    "        return\n",
    "\n",
    "    print(\"--- 每种氨基酸对的最常见原子配对方式 (已去重) ---\")\n",
    "    \n",
    "\n",
    "    # 写入 result_file 结果文件\n",
    "    try:\n",
    "        with open(result_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['res1', 'res2', 'atom1', 'atom2', 'avg_distance','count'])\n",
    "            \n",
    "            sorted_res_pairs = sorted(pair_data.items())\n",
    "            \n",
    "            for res_pair, atom_pair_details in sorted_res_pairs:\n",
    "                first_res_pair =True\n",
    "                if not atom_pair_details:\n",
    "                    continue\n",
    "\n",
    "                # 根据 'count' 键查找最常见的原子对\n",
    "                #most_common_atom_pair = max(atom_pair_details, key=lambda k: atom_pair_details[k]['count'])\n",
    "                for key,val in atom_pair_details.items():\n",
    "                    #data = atom_pair_details[most_common_atom_pair]\n",
    "                    data = val\n",
    "                    count = data['count']\n",
    "                    total_distance = data['total_distance']\n",
    "                    \n",
    "                    # 过滤掉出现次数过少的结果\n",
    "                    if count <= 3:\n",
    "                        continue\n",
    "                    if not first_res_pair and 'C' not in key and 'N' not in key:\n",
    "                        continue\n",
    "\n",
    "                    avg_distance = total_distance / count\n",
    "                    \n",
    "                    # 写入文件\n",
    "                    writer.writerow([\n",
    "                        res_pair[0], \n",
    "                        res_pair[1], \n",
    "                        key[0], \n",
    "                        key[1], \n",
    "                        f\"{avg_distance:.2f}\",\n",
    "                        count,\n",
    "                    ])\n",
    "                    first_res_pair = False\n",
    "                    # 控制台输出\n",
    "                    total_occurrences = sum(d['count'] for d in atom_pair_details.values())\n",
    "                    print(f\"\\n氨基酸对: {res_pair[0]} - {res_pair[1]} (总出现次数: {total_occurrences})\")\n",
    "                    print(f\"  -> 最常见的原子配对: {res_pair[0]}[{key[0]}] - {res_pair[1]}[{key[1]}] (出现 {count} 次, 平均距离: {avg_distance:.2f} Å)\")\n",
    "        \n",
    "        print(f\"\\n[成功] 分析结果已写入: {result_file}\")\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"\\n[错误] 无法写入结果文件: {e}\")\n",
    "    \n",
    "    return pair_data\n",
    "# 调用函数\n",
    "file_path = '/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/link3_data.csv'\n",
    "cleaned_chain_dir = '/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/tmp/cluster.tsv'\n",
    "pair_res = analyze_atom_pairings(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "270cd21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('CG', 'CD1'): 4,\n",
       "             ('CG', 'CG'): 4,\n",
       "             ('CD1', 'CG'): 4,\n",
       "             ('CD1', 'CE2'): 4,\n",
       "             ('CD2', 'CZ'): 4,\n",
       "             ('CD2', 'CD2'): 4,\n",
       "             ('CD2', 'CE2'): 4,\n",
       "             ('CE1', 'CD2'): 2,\n",
       "             ('CE1', 'CE2'): 4,\n",
       "             ('CE2', 'CD1'): 4,\n",
       "             ('CE2', 'CE1'): 4,\n",
       "             ('CE2', 'CD2'): 4,\n",
       "             ('CZ', 'CD2'): 4,\n",
       "             ('CD2', 'CE1'): 2})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_res[('PHE','PHE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8449fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA\n",
      "BBB\n",
      "CCC\n"
     ]
    }
   ],
   "source": [
    "from Bio.PDB import MMCIFParser, PDBParser, PDBIO, MMCIFIO, Select\n",
    "cif_id = \"6QSW.cif\"\n",
    "cif_path = os.path.join(\"/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/CIF_ALL_DATASET\", cif_id )\n",
    "seq_records = []\n",
    "# 加载 CIF 文件（自动处理 .gz 压缩）\n",
    "try:\n",
    "    parser = MMCIFParser(QUIET=True)\n",
    "    if cif_path.endswith(\".gz\"):\n",
    "        with gzip.open(cif_path, 'rt') as f:\n",
    "            structure = parser.get_structure(cif_id, f)\n",
    "    else:\n",
    "        structure = parser.get_structure(cif_id, cif_path)\n",
    "except Exception as e:\n",
    "    print(f\"[未知错误] 加载失败: {str(e)} 在{cif_path}\")\n",
    "\n",
    "# 取第一个模型（针对多模型 CIF 文件）\n",
    "if len(structure) == 0:\n",
    "    pass\n",
    "model = structure[0]\n",
    "\n",
    "# 遍历处理每个链\n",
    "for chain in model:\n",
    "    print(chain.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
