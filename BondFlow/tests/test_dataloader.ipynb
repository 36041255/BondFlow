{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83224d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from BondFlow.data.dataloader import get_dataloader\n",
    "# from BondFlow.models.adapter import build_plm_encoder\n",
    "from BondFlow.models.Loss import *\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import BondFlow.data.SM_utlis as smu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bcdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BondFlow.data import utils as iu\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "import numpy as np\n",
    "import random\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from BondFlow.models.mymodel import *\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "def make_deterministic(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "#conf.inference.ckpt_override_path= \"../Train/weights_base3/best_model.pth\"\n",
    "config_file = \"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/config/base.yaml\"\n",
    "config_path = os.path.dirname(config_file)\n",
    "config_path = os.path.relpath(config_path)\n",
    "config_name = os.path.basename(config_file).split(\".yaml\")[0]\n",
    "\n",
    "# 创建模型 + DDP包装 \n",
    "with initialize(version_base=None, config_path=config_path):\n",
    "    conf = compose(config_name=config_name)\n",
    "\n",
    "# 调用主函数\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "# log = logging.getLogger(__name__)\n",
    "# if conf.inference.deterministic:\n",
    "#     make_deterministic()\n",
    "\n",
    "# # Check for available GPU and print result of check\n",
    "# if torch.cuda.is_available():\n",
    "#     device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "#     log.info(f\"Found GPU with device_name {device_name}. Will run RFdiffusion on {device_name}\")\n",
    "# else:\n",
    "#     log.info(\"////////////////////////////////////////////////\")\n",
    "#     log.info(\"///// NO GPU DETECTED! Falling back to CPU /////\")\n",
    "#     log.info(\"////////////////////////////////////////////////\")\n",
    "\n",
    "# # Initialize sampler and target/contig.\n",
    "# sampler = MySampler(conf,device=device)  # 使用传入的 device 参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "564b6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BondFlow.models.interpolant import _uniform_so3 as interpolant_uniform_so3\n",
    "from multiflow_data import utils as du\n",
    "from multiflow_data.so3_utils import sample_uniform as so3_sample_uniform\n",
    "def _center_xyz_with_chain_and_hotspots(\n",
    "    xyz_target: torch.Tensor,\n",
    "    res_mask: torch.Tensor,\n",
    "    str_mask: torch.Tensor,\n",
    "    chain_ids: torch.Tensor | None = None,\n",
    "    hotspots: torch.Tensor | None = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    根据链信息与 hotspot / 固定结构掩码对坐标进行中心化。\n",
    "\n",
    "    规则：\n",
    "        - 单链：有固定结构 (str_mask=False) 时，以其 CA 几何中心为原点；否则用整体 CA 中心。\n",
    "        - 多链：若提供 hotspot，则以 hotspot CA 平均中心为原点；\n",
    "                否则用固定结构部分的 CA 中心；\n",
    "                若仍不存在，则退化为整体 CA 中心。\n",
    "    \"\"\"\n",
    "    B, L, _, _ = xyz_target.shape\n",
    "    device = xyz_target.device\n",
    "    dtype = xyz_target.dtype\n",
    "\n",
    "    # ---- 1) 判定每个样本是单链还是多链 (batch 张量操作) ----\n",
    "    if chain_ids is None:\n",
    "        single_flag = torch.ones(B, 1, dtype=torch.bool, device=device)\n",
    "    else:\n",
    "        valid = chain_ids > 0  # 忽略 0 这一类占位 id\n",
    "        has_valid = valid.any(dim=1, keepdim=True)  # [B,1]\n",
    "\n",
    "        # 为 masked min/max 构造大/小哨兵值\n",
    "        big_pos = torch.iinfo(chain_ids.dtype).max\n",
    "        big_neg = torch.iinfo(chain_ids.dtype).min\n",
    "        chain_min = torch.where(\n",
    "            valid, chain_ids, torch.full_like(chain_ids, big_pos)\n",
    "        ).amin(dim=1, keepdim=True)\n",
    "        chain_max = torch.where(\n",
    "            valid, chain_ids, torch.full_like(chain_ids, big_neg)\n",
    "        ).amax(dim=1, keepdim=True)\n",
    "\n",
    "        multi_flag = has_valid & (chain_max != chain_min)\n",
    "        single_flag = ~multi_flag  # [B,1]\n",
    "\n",
    "    # ---- 2) 单链 / 多链下的中心掩码 (全张量实现) ----\n",
    "    fixed_mask = (~str_mask.bool()) & res_mask.bool()  # [B,L]\n",
    "    fixed_any = fixed_mask.any(dim=1, keepdim=True)  # [B,1]\n",
    "\n",
    "    # 单链情形的中心掩码：优先固定结构，否则整体\n",
    "    center_single = torch.where(fixed_any, fixed_mask, res_mask.bool())  # [B,L]\n",
    "\n",
    "    # 多链情形：优先 hotspot，其次固定结构，否则整体\n",
    "    if hotspots is not None:\n",
    "        if hotspots.dtype == torch.bool:\n",
    "            h_mask = hotspots & res_mask.bool()\n",
    "        else:\n",
    "            h_mask = (hotspots > 0) & res_mask.bool()\n",
    "        h_any = h_mask.any(dim=1, keepdim=True)  # [B,1]\n",
    "        center_multi = torch.where(h_any, h_mask, center_single)  # [B,L]\n",
    "    else:\n",
    "        center_multi = center_single\n",
    "\n",
    "    # 根据 single_flag 在单链 / 多链规则之间选择\n",
    "    center_mask = torch.where(single_flag, center_single, center_multi)  # [B,L]\n",
    "\n",
    "    # 极端情况：某个样本 center_mask 全 False，则退化为“全部残基”\n",
    "    valid_any = center_mask.any(dim=1, keepdim=True)  # [B,1]\n",
    "    center_mask = torch.where(\n",
    "        valid_any, center_mask, torch.ones_like(center_mask)\n",
    "    )  # [B,L]\n",
    "\n",
    "    # ---- 3) 基于 center_mask 计算几何中心并平移 ----\n",
    "    ca_coords = xyz_target[:, :, 1, :]  # [B, L, 3]\n",
    "    mask_f = center_mask.unsqueeze(-1).to(dtype)  # [B, L, 1]\n",
    "    denom = mask_f.sum(dim=1, keepdim=True) + 1e-8  # [B, 1, 1]\n",
    "    center = (ca_coords * mask_f).sum(dim=1, keepdim=True) / denom  # [B, 1, 3]\n",
    "\n",
    "    # 扩展到原始维度 [B, 1, 1, 3] 以便从所有原子坐标中减去\n",
    "    xyz_centered = xyz_target - center.unsqueeze(2)\n",
    "\n",
    "    return xyz_centered\n",
    "    \n",
    "def sample_with_interpolant(\n",
    "    xyz_target,\n",
    "    res_mask,\n",
    "    str_mask,\n",
    "    hotspots,\n",
    "    t,\n",
    "    head_mask=None,\n",
    "    tail_mask=None,\n",
    "    N_C_anchor=None,\n",
    "    chain_ids=None,\n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    使用 Interpolant 对输入进行加噪或采样。\n",
    "\n",
    "    Args:\n",
    "        xyz_target (Tensor): (B, L, 3, 3) 目标坐标 (N, CA, C)。\n",
    "        seq_target (Tensor): (B, L) 目标序列。\n",
    "        ss_target (Tensor): (B, L, L) 目标二级结构。\n",
    "        res_mask (Tensor): (B, L) 氨基酸残基掩码。\n",
    "        str_mask (Tensor): (B, L) 结构掩码 (True=可扰动, False=固定结构)。\n",
    "        seq_mask (Tensor): (B, L) 序列掩码。\n",
    "        bond_diffuse_mask (Tensor): (B, L, L) 二级结构扩散掩码。\n",
    "        pdb_idx (list): (为了兼容保留, 不再用于识别链)。\n",
    "        t (Tensor): (B,) 扩散时间步。\n",
    "        chain_ids (Tensor, optional): (B, L) full_chain_ids, 用于区分单链/多链。\n",
    "        hotspots (Tensor, optional): (B, L) hotspot 掩码 (>0 视为 True)。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含加噪/采样后数据的字典。\n",
    "    \"\"\"\n",
    "    # 1. 中心化 + 随机旋转 & 平移扰动\n",
    "    #\n",
    "    #   - 单链: 若存在固定结构 (str_mask=False), 以其 CA 几何中心为原点; 否则用整体几何中心。\n",
    "    #   - 多链: 若提供 hotspot, 以 hotspot CA 平均中心为原点; 否则用固定结构部分的 CA 中心;\n",
    "    #           若均不存在则退化为整体几何中心。\n",
    "    xyz_centered = _center_xyz_with_chain_and_hotspots(\n",
    "        xyz_target, res_mask, str_mask, chain_ids=chain_ids, hotspots=hotspots\n",
    "    )\n",
    "    B = xyz_centered.shape[0]\n",
    "    device = xyz_centered.device\n",
    "    dtype = xyz_centered.dtype\n",
    "\n",
    "    # 随机旋转 (batch-wise)\n",
    "    R = so3_sample_uniform(B).to(device=device, dtype=dtype)  # [B, 3, 3]\n",
    "\n",
    "    xyz_rot = torch.einsum(\"bij,blkj->blki\", R, xyz_centered)\n",
    "\n",
    "    # 各向同性平移扰动\n",
    "    epsilon = torch.randn(B, 1, 1, 3, device=device, dtype=dtype) * 1.5\n",
    "    xyz_centered = xyz_rot + epsilon\n",
    "    # 2. 从坐标计算旋转矩阵\n",
    "    #rotmats = iu.get_R_from_xyz(xyz_centered)\n",
    "\n",
    "    # 3. 调用 interpolant.corrupt_batch\n",
    "    # noised_batch = self.interpolant.corrupt_batch(\n",
    "    #     trans_1=xyz_centered[:, :, 1, :],\n",
    "    #     rotmats_1=rotmats,\n",
    "    #     aatypes_1=seq_target,\n",
    "    #     ss_1=ss_target,\n",
    "    #     res_mask=res_mask,\n",
    "    #     trans_diffuse_mask=str_mask.float(),\n",
    "    #     rots_diffuse_mask=str_mask.float(),\n",
    "    #     aatypes_diffuse_mask=seq_mask.float(),\n",
    "    #     ss_diffuse_mask=bond_diffuse_mask.float(),\n",
    "    #     t=t\n",
    "    # )\n",
    "\n",
    "    #xyz_noised = iu.get_xyz_from_RT(noised_batch['rotmats_t'],noised_batch['trans_t'])\n",
    "    \n",
    "    # # 将head_mask和tail_mask位置的张量分别复制为锚定Body残基的特征\n",
    "    # if (head_mask is not None or tail_mask is not None) and (N_C_anchor is not None):\n",
    "    #     noised_batch['aatypes_t'] = iu.update_nc_node_features(noised_batch['aatypes_t'], N_C_anchor, head_mask, tail_mask)\n",
    "    #     # 使用带offset的坐标更新函数\n",
    "    #     xyz_noised = iu.update_nc_node_coordinates(xyz_noised, N_C_anchor, head_mask, tail_mask)\n",
    "    #     xyz_centered = iu.update_nc_node_coordinates(xyz_centered, N_C_anchor, head_mask, tail_mask)\n",
    "    \n",
    "    return xyz_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d92d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 69689 clusters (combined).\n",
      "self.num_replicas None\n",
      "self.rank None\n",
      "self.total_size 69689\n",
      "Validation set: 1789 clusters (combined).\n",
      "self.num_replicas None\n",
      "self.rank None\n",
      "self.total_size 1789\n",
      "69689 1789\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "pdb_list_path = \"/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/train_data5/LINKAF_tmp/cluster.tsv\"\n",
    "pdb_dir = \"/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/train_data5/LINKAF_CIF\"\n",
    "\n",
    "pdb_term_list_path=\"/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/train_data5/LINK_TERM_MONO_tmp/cluster.tsv\"\n",
    "pdb_term_dir=\"/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/train_data5/LINK_TERM_MONO_CIF\"\n",
    "\n",
    "pdb_com_list_path = \"/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/train_data5/COMPLEX_tmp/cluster.tsv\"\n",
    "pdb_com_dir = \"/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/train_data5/COMPLEX_CIF\"\n",
    "\n",
    "\n",
    "pdb_list_paths = [pdb_list_path,pdb_term_list_path,pdb_com_list_path]\n",
    "pdb_dirs = [pdb_dir,pdb_term_dir,pdb_com_dir]\n",
    "\n",
    "# plm_encoder = build_plm_encoder(sampler.model._folding_model,sampler.model._plm_type, device='cuda:0')\n",
    "\n",
    "cache_dir = \"/home/fit/lulei/WORK/xjt/Protein_design/CyclicPeptide/Dataset/ALL_MMCIF/data_cache/\"\n",
    "dataloader_train,dataloader_val = get_dataloader(conf, batch_size, pdb_list_path=pdb_list_paths, pdb_dir=pdb_dirs,\n",
    "                                                    distributed=False, num_workers=0, \n",
    "                                                crop_length=264, device='cpu',rank=None, num_replicas=None,seed=44,\n",
    "                                                val_split=0.025,sampling_ratios=[{'monomer': 1},{'monomer': 1},{'complex_space': 1}],\n",
    "                                                dataset_probs=[1, 0.00,0],samples_num=None,\n",
    "                                                cache_dir = cache_dir,        \n",
    "                                                nc_pos_prob=0.3, hotspot_prob=0.6,)\n",
    "                                                # plm_encoder = plm_encoder,\n",
    "                                                # plm_max_chain_length=1024,)\n",
    "\n",
    "\n",
    "\n",
    "print(len(dataloader_train),len(dataloader_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be1eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataloader_train),len(dataloader_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216524c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# params = count_parameters(sampler.model)\n",
    "# print(f\"模型的参数总量: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77610a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_tensor_heatmap(tensor, title=\"Tensor Heatmap\", cmap=\"viridis\"):\n",
    "    \"\"\"\n",
    "    绘制L×L tensor矩阵的热图\n",
    "    \n",
    "    参数:\n",
    "    tensor -- 输入的L×L矩阵\n",
    "    title -- 热图标题(可选)\n",
    "    cmap -- 颜色映射(可选)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # 显示热图\n",
    "    im = plt.imshow(tensor, cmap=cmap)\n",
    "    \n",
    "    # 添加颜色条\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 设置标题和坐标轴\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Column Index\")\n",
    "    plt.ylabel(\"Row Index\")\n",
    "    \n",
    "    # 显示网格线(可选)\n",
    "    plt.grid(False)\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75dba6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contig [['A/38-A/299:seq_PNA:str_PNA']]\n",
      "hotspots []\n"
     ]
    }
   ],
   "source": [
    "batch_data = None\n",
    "dataloader_train.sampler.set_epoch(7)\n",
    "for i, batch in enumerate(dataloader_train):\n",
    "    batch_data = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2031ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AF-A0RYW9-F1-model_v4_A']\n"
     ]
    }
   ],
   "source": [
    "print(batch_data['pdb_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da4a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 264, 14, 3])\n"
     ]
    }
   ],
   "source": [
    "xyz_target = batch_data['full_xyz']\n",
    "rf_idx = batch_data['full_rf_idx']\n",
    "res_mask = batch_data['res_mask']\n",
    "seq_target = batch_data['full_seq']\n",
    "xyz_center = sample_with_interpolant(xyz_target,batch_data['res_mask'],batch_data['full_mask_str'],\n",
    "                                        batch_data['full_hotspot'],0,\n",
    "                                        chain_ids=batch_data['full_chain_ids'])\n",
    "B,L=xyz_target.shape[:2]\n",
    "xyz_nan = torch.full((B, L, 14, 3), float('nan'))\n",
    "xyz_nan[:,:,:4,:] = xyz_center[:,:,:4,:]\n",
    "xyz_nan[:,-1,:,:] = float('nan')\n",
    "xyz_nan[:,0,:,:] = float('nan')\n",
    "# RTframes,allatom_xyz_withCN = sampler.allatom(seq_pred,xyz_pred[:,-1,...],\n",
    "#                             alpha_s,use_H=False,bond_mat=bond_matrix_sampled,\n",
    "#                             link_csv_path=\"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/config/link.csv\",\n",
    "#                             res_mask=res_mask)\n",
    "\n",
    "# RTframes,allatom_xyz_true = sampler.allatom(seq_target,xyz_centered,\n",
    "#                         alpha_target,use_H=False,bond_mat=bond_matrix_target,\n",
    "#                         link_csv_path=\"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/config/link.csv\",\n",
    "#                         res_mask=final_res_mask)\n",
    "\n",
    "\n",
    "from rfdiff.util import writepdb\n",
    "import numpy as np\n",
    "\n",
    "# Get the first sample from the batch\n",
    "atoms_to_save = xyz_nan[0].cpu()\n",
    "seq_to_save = seq_target[0].cpu()\n",
    "res_mask_to_save = res_mask[0].cpu().numpy()\n",
    "\n",
    "# Filter atoms and sequence based on residue mask\n",
    "chain_pdb_idx = [res[0] for res in batch_data['full_origin_pdb_idx'][0]]\n",
    "L = atoms_to_save.shape[0]\n",
    "res_indices = np.arange(L)[res_mask_to_save.astype(bool)]\n",
    "filtered_atoms = atoms_to_save[res_indices]\n",
    "filtered_seq = seq_to_save[res_indices]\n",
    "\n",
    "# Prepare arguments for writepdb\n",
    "outfile = \"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/tests/output.pdb\"\n",
    "res = filtered_seq\n",
    "ch = ['A'] * len(filtered_seq) # Assuming a single chain 'A'\n",
    "resi = res_indices + 1 # PDB residue indices are 1-based\n",
    "\n",
    "# Write the PDB file\n",
    "writepdb(outfile, filtered_atoms, res,chain_idx=chain_pdb_idx,idx_pdb=rf_idx[0])\n",
    "\n",
    "\n",
    "# Get the first sample from the batch\n",
    "print(xyz_target.shape)\n",
    "xyz_target[:,-1,:,:] = float('nan')\n",
    "xyz_target[:,0,:,:] = float('nan')\n",
    "atoms_to_save = xyz_target[:,:,:4,:][0].cpu()\n",
    "\n",
    "seq_to_save = seq_target[0].cpu()\n",
    "res_mask_to_save = res_mask[0].cpu().numpy()\n",
    "\n",
    "# Filter atoms and sequence based on residue mask\n",
    "chain_pdb_idx = [res[0] for res in batch_data['full_origin_pdb_idx'][0]]\n",
    "L = atoms_to_save.shape[0]\n",
    "res_indices = np.arange(L)[res_mask_to_save.astype(bool)]\n",
    "filtered_atoms = atoms_to_save[res_indices]\n",
    "filtered_seq = seq_to_save[res_indices]\n",
    "\n",
    "# Prepare arguments for writepdb\n",
    "outfile = \"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/tests/output_orig.pdb\"\n",
    "res = filtered_seq\n",
    "ch = ['A'] * len(filtered_seq) # Assuming a single chain 'A'\n",
    "resi = res_indices + 1 # PDB residue indices are 1-based\n",
    "\n",
    "# Write the PDB file\n",
    "writepdb(outfile, filtered_atoms, res,chain_idx=chain_pdb_idx,idx_pdb=rf_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1029c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "print(batch_data['full_mask_str'][batch_index])\n",
    "print(batch_data['full_mask_seq'][batch_index])\n",
    "print(batch_data['res_mask'][batch_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_data.keys())\n",
    "print(batch_data['full_pdb_idx'])\n",
    "print(batch_data['full_origin_pdb_idx'])\n",
    "print(batch_data['full_seq'].shape)\n",
    "print(batch_data['pdb_id'])\n",
    "print(batch_data['full_hotspot'].shape)\n",
    "idx = torch.where(batch_data['full_hotspot']==1)\n",
    "print(idx)\n",
    "b_idx, r_idx = idx\n",
    "\n",
    "for b, r in zip(b_idx.tolist(), r_idx.tolist()):\n",
    "    print(batch_data['full_origin_pdb_idx'][b][r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91497a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "print(batch_data['full_seq'][batch_index])\n",
    "print(batch_data['full_head_mask'][batch_index])\n",
    "print(batch_data['full_tail_mask'][batch_index])\n",
    "print(batch_data['full_rf_idx'][batch_index])\n",
    "N_index = 1#torch.where(batch_data['full_N_C_anchor'][0,:,:,0]==1)[0].max()\n",
    "C_index =260 #`torch.where(batch_data['full_N_C_anchor'][0,:,:,1]==1)[0].min()\n",
    "N_vuratal_index = 0#torch.where(batch_data['full_N_C_anchor'][0,:,:,0]==1)[0].min()\n",
    "C_vuratal_index = 260#torch.where(batch_data['full_N_C_anchor'][0,:,:,1]==1)[0].max()\n",
    "print(N_index,C_index)\n",
    "print(torch.where(batch_data['full_N_C_anchor'][batch_index,:,:,1]==1))\n",
    "print(torch.where(batch_data['full_N_C_anchor'][batch_index,:,:,0]==1))\n",
    "print(f\"N residue {N_index}\",batch_data['full_xyz'][batch_index,N_index,:3,:])\n",
    "print(f\"C residue {C_index}\",batch_data['full_xyz'][batch_index,C_index,:3,:])\n",
    "print(f\"N virtual residue {N_vuratal_index}\",batch_data['full_xyz'][batch_index,N_vuratal_index,:3,:])\n",
    "print(f\"C virtual residue {C_vuratal_index}\",batch_data['full_xyz'][batch_index,C_vuratal_index,:3,:])\n",
    "print((((batch_data['full_xyz'][batch_index,C_vuratal_index,1,:]-batch_data['full_xyz'][batch_index,N_vuratal_index,1,:])**2).sum())**0.5)\n",
    "print(\"bond matrix == 1 的索引对:\",torch.where(batch_data['full_bond_matrix'][batch_index] * (1-torch.eye(batch_data['full_bond_matrix'][0].shape[1],dtype=torch.float32,device=batch_data['full_bond_matrix'][0].device))==1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b178b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx =0\n",
    "print(torch.where(batch_data['full_bond_matrix'][batch_idx] * (1-torch.eye(batch_data['full_bond_matrix'][0].shape[1],dtype=torch.float32,device=batch_data['full_bond_matrix'][0].device).unsqueeze(0))==1))\n",
    "plot_tensor_heatmap(batch_data['full_bond_matrix'][batch_idx],\"bond mat\")\n",
    "plot_tensor_heatmap(batch_data['full_bond_mask'][batch_idx], \"bond mask\")\n",
    "plot_tensor_heatmap(batch_data['full_N_C_anchor'][batch_idx,:,:,0], \"full_N_C_anchor N\")\n",
    "print(torch.where(batch_data['full_N_C_anchor'][batch_idx,:,:,0]==1))\n",
    "plot_tensor_heatmap(batch_data['full_N_C_anchor'][batch_idx,:,:,1], \"full_N_C_anchor C\")\n",
    "print(torch.where(batch_data['full_N_C_anchor'][batch_idx,:,:,1]==1))\n",
    "print(torch.where(batch_data['full_tail_mask'][batch_idx,:]==1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fa6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_nan_loss(loss_tensor, loss_name, pdb_id, log_file_path):\n",
    "    \"\"\"\n",
    "    Logs a message if the loss is NaN.\n",
    "\n",
    "    Args:\n",
    "        loss_tensor (torch.Tensor): The loss value.\n",
    "        loss_name (str): The name of the loss (e.g., 'frame', 'seq').\n",
    "        batch_data (dict): The batch data dictionary, containing 'pdb_id'.\n",
    "        log_file_path (str): The path to the log file.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the loss was NaN, False otherwise.\n",
    "    \"\"\"\n",
    "    if torch.isnan(loss_tensor):\n",
    "        with open(log_file_path, 'a') as f:\n",
    "            pdb_ids = \" \".join(pdb_id)\n",
    "            log_message = f\"{loss_name} nan {pdb_ids} {loss_tensor.item()}\\\\n\"\n",
    "            f.write(log_message)\n",
    "            print(log_message)\n",
    "        return True\n",
    "    return False\n",
    "log_file_path = \"/home/fit/lulei/WORK/xjt/Protein_design/RFdiffusion/mytest/tests/train_log.txt\"\n",
    "\n",
    "\n",
    "def print_gpu_memory(loss_name):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        print(f'GPU Memory after {loss_name}: {torch.cuda.memory_allocated() / 1024**2:.2f} MB')\n",
    "\n",
    "def model_forward(sampler,batch_data,criterion_frame, criterion_seq, \n",
    "                                                criterion_bond,\n",
    "                                                criterion_FAPE,\n",
    "                                                criterion_torsion,\n",
    "                                                criterion_bond_coh,\n",
    "                                                eps_t = 5e-3,\n",
    "                                                criterion_clash=None):\n",
    "    \n",
    "    # 1. Extract data from batch and move to device\n",
    "    xyz_orig = batch_data['full_xyz'].to(sampler.device) # (B, L, 14, 3)\n",
    "    seq_target = batch_data['full_seq'].to(sampler.device)\n",
    "    bond_matrix_target = batch_data['full_bond_matrix'].to(sampler.device)\n",
    "    res_mask = batch_data['res_mask'].to(sampler.device)\n",
    "    str_mask = batch_data['full_mask_str'].to(sampler.device)\n",
    "    seq_mask = batch_data['full_mask_seq'].to(sampler.device)\n",
    "    bond_mask = batch_data['full_bond_mask'].to(sampler.device)\n",
    "    head_mask = batch_data['full_head_mask'].to(sampler.device)\n",
    "    tail_mask = batch_data['full_tail_mask'].to(sampler.device)\n",
    "    pdb_idx = batch_data['full_pdb_idx']\n",
    "    chain_ids = batch_data['full_chain_ids']\n",
    "    rf_idx = batch_data['full_rf_idx'].to(sampler.device)\n",
    "    alpha_target = batch_data['full_alpha'].to(sampler.device)\n",
    "    alpha_alt_target = batch_data['full_alpha_alt'].to(sampler.device)  # [L,10,2] for alpha torsions, [L,10,2] for alpha torsion alt\n",
    "    alpha_tor_mask = batch_data['full_alpha_tor_mask'].to(sampler.device)\n",
    "    pdb_id = batch_data['pdb_id']\n",
    "    N_C_anchor = batch_data['full_N_C_anchor'].to(sampler.device)\n",
    "    final_res_mask = res_mask.float() * (1 - head_mask.float()) * (1 - tail_mask.float())\n",
    "        # 新增：原始 PDB 信息（由 dataloader 提供，用于整链 PLM）\n",
    "    origin_pdb_idx = batch_data['full_origin_pdb_idx']  # list[list[(chain,res)]]\n",
    "    pdb_seq_full = batch_data['pdb_seq_full']           # list[np.ndarray 或 1D tensor]\n",
    "    pdb_idx_full = batch_data['pdb_idx_full']           # list[list[(chain,res)]]\n",
    "    pdb_core_id = batch_data['pdb_core_id']             # list[str]\n",
    "    hotspots = batch_data['full_hotspot'].to(sampler.device)\n",
    "\n",
    "    print_gpu_memory('data to device')\n",
    "\n",
    "    \n",
    "    # Create dummy tensors for ss since it's not in batch_data\n",
    "    B, L = seq_target.shape\n",
    "\n",
    "    # 2. Get random timestep\n",
    "    partial_T = torch.rand((1,), device=sampler.device).repeat(B) * (1 - eps_t) \n",
    "    print(\"partial_T\",partial_T)\n",
    "    # 3. Noise the data using interpolant's sample method\n",
    "    xyz_noised, seq_noised, bond_noised,xyz_centered,rotmats =  sampler.sample_with_interpolant(xyz_orig[:,:,:3,:], \n",
    "                                                                            seq_target, \n",
    "                                                                            bond_matrix_target, \n",
    "                                                                            res_mask, \n",
    "                                                                            str_mask, \n",
    "                                                                            seq_mask,\n",
    "                                                                            bond_mask, \n",
    "                                                                            hotspots, \n",
    "                                                                            partial_T,\n",
    "                                                                            head_mask=head_mask,\n",
    "                                                                            tail_mask=tail_mask,\n",
    "                                                                            N_C_anchor=N_C_anchor)\n",
    "    print(\"xyz_noised 0\",xyz_noised[0,0,...])\n",
    "    print(\"xyz_noised 1\",xyz_noised[0,1,...])\n",
    "\n",
    "    print_gpu_memory('after sample_with_interpolant')\n",
    "\n",
    "    # 4. Preprocess batch for the model\n",
    "\n",
    "    # 4. Preprocess batch for the model\n",
    "    # plot_tensor_heatmap(bond_noised[0].cpu(), title=\"Noised Bond Matrix\")\n",
    "    # print(bond_noised[0][(bond_noised[0]>0)& (bond_noised[0]<1)])\n",
    "    # plot_tensor_heatmap(bond_noised[1].cpu(), title=\"Noised Bond Matrix\")\n",
    "    # plot_tensor_heatmap(bond_matrix_target[0].cpu(), title=\"Bond Matrix\")\n",
    "    # plot_tensor_heatmap(bond_matrix_target[1].cpu(), title=\"Bond Matrix\")\n",
    "    # print(bond_noised[1][(bond_noised[1]>0)& (bond_noised[1]<1)])\n",
    "    # plot_tensor_heatmap(bond_mask[0].cpu(), title=\"Bond mask\")\n",
    "    # plot_tensor_heatmap(bond_mask[1].cpu(), title=\"Bond mask\")\n",
    "    \n",
    "    print_gpu_memory('after _preprocess_batch')\n",
    "    # plot_tensor_heatmap(res_dist_matrix[0].cpu(), title=\"CA\")\n",
    "    # plot_tensor_heatmap(res_dist_matrix[1].cpu(), title=\"CA Dist Matrix\")\n",
    "    # 5. Model forward pass\n",
    "    start_time = time.time()\n",
    "        \n",
    "    logits_aa, xyz_pred, alpha_s, bond_matrix = sampler.model(\n",
    "            seq_noised=seq_noised,\n",
    "            xyz_noised=xyz_noised,\n",
    "            bond_noised=bond_noised,\n",
    "            rf_idx=rf_idx,\n",
    "            pdb_idx=pdb_idx,\n",
    "            alpha_target=alpha_target,\n",
    "            alpha_tor_mask=alpha_tor_mask,\n",
    "            partial_T=partial_T,\n",
    "            str_mask=str_mask,\n",
    "            seq_mask=seq_mask,\n",
    "            N_C_anchor=N_C_anchor,\n",
    "            head_mask=head_mask,\n",
    "            tail_mask=tail_mask,\n",
    "            bond_mask=bond_mask,\n",
    "            res_mask=res_mask,\n",
    "            use_checkpoint=False,\n",
    "            trans_1=xyz_centered[:,:,1,:].to(xyz_noised.dtype),\n",
    "            rotmats_1= rotmats.to(xyz_noised.dtype),\n",
    "            aatypes_1=seq_target.long(),\n",
    "            bond_mat_1=bond_matrix_target,\n",
    "            chain_ids=chain_ids,\n",
    "            origin_pdb_idx=origin_pdb_idx,\n",
    "            pdb_seq_full=pdb_seq_full,\n",
    "            pdb_idx_full=pdb_idx_full,\n",
    "            pdb_core_id=pdb_core_id,\n",
    "            hotspots=hotspots,\n",
    "        )\n",
    "\n",
    "    seq_pred = torch.argmax(logits_aa, dim=-1)\n",
    "\n",
    "    seq_noised_mask = (seq_noised == du.MASK_TOKEN_INDEX)\n",
    "    seq_res_mask = seq_mask.float() * final_res_mask.float() * seq_noised_mask.float()\n",
    "    seq_pred = seq_pred * seq_res_mask.float() + (1 - seq_res_mask.float()) * seq_target.long()\n",
    "\n",
    "    str_res_mask = str_mask.float() * res_mask.float() \n",
    "    times = xyz_pred.size(1)\n",
    "    # Reshape str_res_mask to broadcast with xyz_pred: (B, L) -> (B, 1, L, 1, 1)\n",
    "    str_res_mask_expanded = str_res_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
    "    xyz_pred = xyz_pred * str_res_mask_expanded + ( 1 - str_res_mask_expanded) * xyz_centered.unsqueeze(1).expand(-1,times,-1,-1,-1)[...,:3,:]\n",
    "\n",
    "\n",
    "    print(\"model forward time:\", time.time() - start_time)\n",
    "    print_gpu_memory('after model forward pass')\n",
    "    \n",
    "    # return logits_2d, logits_aa, logits_exp, xyz_pred, alpha_s, bond_matrix, lddt\n",
    "    # 计算frame损失\n",
    "    target_times =xyz_centered.unsqueeze(1).expand(-1,times,-1,-1,-1)[...,:3,:]\n",
    "    noise_times = xyz_noised.unsqueeze(1).expand(-1,times,-1,-1,-1)[...,:3,:]\n",
    "    # print(\"center of xyz_pred:\",xyz_pred[...,1,:].mean(dim=(1,2)))\n",
    "    # print(\"center of xyz_noised:\",noise_times[...,1,:].mean(dim=(1,2)))\n",
    "    # print(\"center of xyz_target:\",target_times[...,1,:].mean(dim=(1,2)))\n",
    "    loss_frame_start = time.time()\n",
    "    loss_frame = criterion_frame(xyz_pred, target_times,noise_times, mask = str_res_mask.bool(),t=partial_T)\n",
    "    log_nan_loss(loss_frame, 'frame', pdb_id, log_file_path)\n",
    "    loss_frame_time = time.time() - loss_frame_start\n",
    "    print(f\"frame loss time: {loss_frame_time}\")\n",
    "    print_gpu_memory('after loss_frame')\n",
    "\n",
    "\n",
    "    loss_FAPE_start = time.time()\n",
    "    loss_FAPE = criterion_FAPE(xyz_pred[:,-1,...],xyz_centered,final_res_mask, str_mask)\n",
    "    #loss_FAPE = criterion_FAPE(xyz_pred,target_times,res_mask, str_mask)\n",
    "    log_nan_loss(loss_FAPE, 'FAPE', pdb_id, log_file_path)\n",
    "    loss_FAPE_time = time.time() - loss_FAPE_start\n",
    "    print(f\"FAPE loss time: {loss_FAPE_time}\")\n",
    "    print_gpu_memory('after loss_FAPE')\n",
    "\n",
    "    loss_seq_start = time.time()    \n",
    "    # 计算序列损失        \n",
    "   \n",
    "    loss_seq = criterion_seq(logits_aa, seq_target, mask=seq_res_mask.bool())\n",
    "    log_nan_loss(loss_seq, 'seq', pdb_id, log_file_path)\n",
    "    loss_seq_time = time.time() - loss_seq_start\n",
    "    print(f\"seq loss time: {loss_seq_time}\")\n",
    "    print_gpu_memory('after loss_seq')\n",
    "\n",
    "    #计算键合矩阵的loss\n",
    "    mask_res_bond_2d = (res_mask.unsqueeze(2).float() * res_mask.unsqueeze(1).float() * bond_mask.float()).bool()\n",
    "    loss_bond_start = time.time()\n",
    "    loss_bond = criterion_bond(bond_matrix, bond_matrix_target, mask_res_bond_2d)\n",
    "    loss_bond_time = time.time() - loss_bond_start\n",
    "    print(f\"bond loss time: {loss_bond_time}\")\n",
    "    print_gpu_memory('after loss_bond')\n",
    "    # plot_tensor_heatmap(bond_matrix_target[0].detach().cpu(), title=\"Bond Matrix\")\n",
    "    # plot_tensor_heatmap(bond_matrix[0].detach().cpu(), title=\"Bond Matrix pred\")\n",
    "\n",
    "    # # 用assert检查 bond_matrix[0] 是否为双随机矩阵（行和列和均接近1）\n",
    "    # # 只在 mask_2d 有效位置检测双随机性\n",
    "    # bm = bond_matrix[0].detach().cpu()\n",
    "    # mask = mask_2d[0].detach().cpu()\n",
    "    # # 为避免全0行/列影响，限定检测在 mask 有效的位置\n",
    "    # row_valid = mask.sum(dim=1) > 0\n",
    "    # col_valid = mask.sum(dim=0) > 0\n",
    "    # row_sums = bm.sum(dim=1)\n",
    "    # col_sums = bm.sum(dim=0)\n",
    "    # assert torch.allclose(row_sums[row_valid], torch.ones_like(row_sums[row_valid]), atol=1e-2), \\\n",
    "    #     f\"Row sums not close to 1: min={row_sums[row_valid].min().item()}, max={row_sums[row_valid].max().item()}\"\n",
    "    # assert torch.allclose(col_sums[col_valid], torch.ones_like(col_sums[col_valid]), atol=1e-2), \\\n",
    "    #     f\"Column sums not close to 1: min={col_sums[col_valid].min().item()}, max={col_sums[col_valid].max().item()}\"\n",
    "    # plot_tensor_heatmap(bond_noised[0].detach().cpu(), title=\"bond_noised\")\n",
    "\n",
    "    t_threshold = 0.25\n",
    "    partial_T_mask = (partial_T > t_threshold)\n",
    "\n",
    "    # 计算clash loss\n",
    "    sapmle_bond_start = time.time()\n",
    "    mask_res_2d = (res_mask.unsqueeze(2).float() * res_mask.unsqueeze(1).float()).bool()\n",
    "    bond_matrix_sampled = smu.sample_permutation(bond_matrix, mask_res_2d)\n",
    "    sample_bond_time = time.time() - sapmle_bond_start\n",
    "    print(f\"sample bond time: {sample_bond_time}\")\n",
    "\n",
    "    allatom_start = time.time()\n",
    " \n",
    "    RTframes,allatom_xyz_withCN = sampler.allatom(seq_pred,xyz_pred[:,-1,...],\n",
    "                                alpha_s,use_H=False,bond_mat=bond_matrix_sampled,\n",
    "                                link_csv_path=\"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/config/link.csv\",\n",
    "                                res_mask=res_mask)\n",
    "\n",
    "    RTframes,allatom_xyz_true = sampler.allatom(seq_target,xyz_centered,\n",
    "                            alpha_target,use_H=False,bond_mat=bond_matrix_target,\n",
    "                            link_csv_path=\"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/config/link.csv\",\n",
    "                            res_mask=final_res_mask)\n",
    "\n",
    "    allatom_time = time.time() - allatom_start\n",
    "\n",
    "    print_gpu_memory('after allatom')\n",
    "\n",
    "\n",
    "    loss_clash_start = time.time()\n",
    "    loss_clash = criterion_clash(N_C_anchor,allatom_xyz_withCN, seq_target, final_res_mask*partial_T_mask[:,None].float(),\n",
    "                                )\n",
    "    log_nan_loss(loss_clash, 'clash', pdb_id, log_file_path)\n",
    "    loss_clash_time = time.time() - loss_clash_start\n",
    "    print(f\"clash loss time: {loss_clash_time}\")\n",
    "    print_gpu_memory('after loss_clash')\n",
    "    # 计算torsion loss\n",
    "    loss_torsion_start = time.time()\n",
    "    alpha_tor_res_mask = alpha_tor_mask.float() * final_res_mask[:,:,None].float()*partial_T_mask[:,None,None].float()\n",
    "    loss_torsion = criterion_torsion(alpha_s, alpha_target, alpha_alt_target, alpha_tor_res_mask,bond_mat=bond_matrix)\n",
    "    log_nan_loss(loss_torsion, 'torsion', pdb_id, log_file_path)\n",
    "    loss_torsion_time = time.time() - loss_torsion_start\n",
    "    print(f\"torsion loss time: {loss_torsion_time}\")\n",
    "    print_gpu_memory('after loss_torsion')\n",
    "    loss_bond_coh_start = time.time()   \n",
    "    loss_bond_coh = criterion_bond_coh(\n",
    "        bond_matrix=bond_matrix,\n",
    "        res_mask=res_mask*partial_T_mask[:,None].float(),\n",
    "        #seq_logits=logits_aa,\n",
    "        true_seq = seq_target,\n",
    "        seq_labels = seq_target,\n",
    "        mask_2d=None,\n",
    "        all_atom_coords = allatom_xyz_withCN,\n",
    "        aatype = seq_target,\n",
    "        t = None,\n",
    "        head_mask=head_mask,\n",
    "        tail_mask=tail_mask,\n",
    "        nc_anchor=N_C_anchor,\n",
    "        # true_bond_matrix=bond_matrix_target.float(),\n",
    "    )\n",
    "    \n",
    "    log_nan_loss(loss_bond_coh, 'bond_coh', pdb_id, log_file_path)\n",
    "    loss_bond_coh_time = time.time() - loss_bond_coh_start\n",
    "    print(f\"bond_coh loss time: {loss_bond_coh_time}\")\n",
    "    print_gpu_memory('after loss_bond_coh')\n",
    "    print(\"model loss-forward time:\", time.time() - start_time)\n",
    "\n",
    "\n",
    "\n",
    "    from rfdiff.util import writepdb\n",
    "    import numpy as np\n",
    "\n",
    "    # Get the first sample from the batch\n",
    "    atoms_to_save = allatom_xyz_withCN[0].cpu()\n",
    "    seq_to_save = seq_target[0].cpu()\n",
    "    res_mask_to_save = res_mask[0].cpu().numpy()\n",
    "\n",
    "    # Filter atoms and sequence based on residue mask\n",
    "    chain_pdb_idx = [res[0] for res in batch_data['full_origin_pdb_idx'][0]]\n",
    "    L = atoms_to_save.shape[0]\n",
    "    res_indices = np.arange(L)[res_mask_to_save.astype(bool)]\n",
    "    filtered_atoms = atoms_to_save[res_indices]\n",
    "    filtered_seq = seq_to_save[res_indices]\n",
    "\n",
    "    # Prepare arguments for writepdb\n",
    "    outfile = \"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/tests/output.pdb\"\n",
    "    res = filtered_seq\n",
    "    ch = ['A'] * len(filtered_seq) # Assuming a single chain 'A'\n",
    "    resi = res_indices + 1 # PDB residue indices are 1-based\n",
    "\n",
    "    # Write the PDB file\n",
    "    writepdb(outfile, filtered_atoms, res,chain_idx=chain_pdb_idx,idx_pdb=rf_idx[0])\n",
    "\n",
    "    print(f\"Saved PDB file to {outfile}\")\n",
    "\n",
    "    outfile = \"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/tests/output_orig.pdb\"\n",
    "    res = seq_target[0].cpu()\n",
    "    atoms_to_save = allatom_xyz_true[0].cpu()\n",
    "    res_indices = np.arange(L)[res_mask_to_save.astype(bool)]\n",
    "    filtered_atoms = atoms_to_save[res_indices]\n",
    "    filtered_seq = seq_to_save[res_indices]\n",
    "    res = filtered_seq\n",
    "    writepdb(outfile, filtered_atoms, res,chain_idx=chain_pdb_idx,idx_pdb=rf_idx[0])\n",
    "\n",
    "    # plot_tensor_heatmap(bond_matrix_target[0].detach().cpu(), title=\"Bond Matrix\")\n",
    "    # plot_tensor_heatmap(bond_matrix[0].detach().cpu(), title=\"Bond Matrix pred\")\n",
    "    # plot_tensor_heatmap(bond_matrix_sampled[0].detach().cpu(), title=\"bond_matrix_sampled\")\n",
    "\n",
    "\n",
    "        # 输出bond_mat中元素1的索引对\n",
    "    bond_indices = torch.nonzero(bond_matrix_target* (1- torch.eye(bond_matrix_target.shape[1],dtype=torch.float32,device=bond_matrix_target.device).unsqueeze(0)) == 1)\n",
    "    print(\"bond_matrix == 1 的索引对:\")\n",
    "    for idx in bond_indices:\n",
    "        # idx is tensor([batch_idx, i, j])\n",
    "        batch_idx, i, j = idx.tolist()\n",
    "        print(f\"batch {batch_idx}: ({i}, {j})\")\n",
    "\n",
    "    return loss_frame, loss_seq, loss_bond, loss_clash,loss_FAPE, loss_torsion, loss_bond_coh,partial_T,  seq_pred, res_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67690094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from rfdiff.util import writepdb\n",
    "# import numpy as np\n",
    "# from rfdiff.util_module import ComputeAllAtomCoords\n",
    "# allatom = ComputeAllAtomCoords()\n",
    "# print(batch_data['full_alpha'].shape)\n",
    "# print(batch_data['full_xyz'].shape)\n",
    "# _,atoms_to_save = allatom(batch_data['full_seq'],\n",
    "#                         batch_data['full_xyz'][:,:,:3,:],\n",
    "#                         batch_data['full_alpha'],use_H=False)\n",
    "\n",
    "# # Get the first sample from the batch\n",
    "# # seq_pred,xyz_pred[:,-1,...],alpha_s,use_H=False\n",
    "# # atoms_to_save = allatom_xyz[0].cpu()\n",
    "# seq_to_save = batch_data['full_seq'][0].cpu()\n",
    "# res_mask_to_save = batch_data['res_mask'][0].cpu().numpy()\n",
    "# atoms_to_save = atoms_to_save[0].cpu()\n",
    "\n",
    "\n",
    "# # Filter atoms and sequence based on residue mask\n",
    "# L = seq_to_save.shape[0]\n",
    "# res_indices = np.arange(L)[res_mask_to_save.astype(bool)]\n",
    "# filtered_atoms = atoms_to_save[res_indices]\n",
    "# filtered_seq = seq_to_save[res_indices]\n",
    "\n",
    "# # Prepare arguments for writepdb\n",
    "# outfile = \"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/tests/output2.pdb\"\n",
    "# res = filtered_seq\n",
    "# ch = ['A'] * len(filtered_seq) # Assuming a single chain 'A'\n",
    "# resi = res_indices + 1 # PDB residue indices are 1-based\n",
    "\n",
    "# # Write the PDB file\n",
    "# writepdb(outfile, filtered_atoms, res)\n",
    "\n",
    "# print(f\"Saved PDB file to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3523251",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(profile='full')\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts\n",
    "import torch.optim  as optim \n",
    "criterion_frame = LFrameLoss(w_trans=0.5, w_rot=1,gamma=1,d_clamp = 25)\n",
    "criterion_seq = LseqLoss()\n",
    "criterion_bond = DSMCrossEntropyLoss()\n",
    "criterion_FAPE = FAPELoss(clamp_distance = 20)\n",
    "criterion_clash = OpenFoldClashLoss(device=device,debug_print_pairs=True)\n",
    "criterion_torsion = TorsionLossLegacy()\n",
    "link_csv_path = \"/home/fit/lulei/WORK/xjt/Protein_design/BondFlow/BondFlow/config/link.csv\"\n",
    "criterion_bond_coh = BondCoherenceLoss(link_csv_path=link_csv_path, device=device,t_geom_threshold=0.5)\n",
    "model = sampler.model\n",
    "\n",
    "def configure_optimizers(model, learning_rate, weight_decay):\n",
    "    decay, no_decay = [], []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if (\n",
    "            param.ndim == 1 or\n",
    "            name.endswith(\".bias\") or\n",
    "            \"layernorm\" in name.lower()\n",
    "        ):\n",
    "            no_decay.append(param)\n",
    "            print(\"no_decay\",name)\n",
    "        else:\n",
    "            decay.append(param)\n",
    "            print(\"decay\",name)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": decay, \"weight_decay\": weight_decay},\n",
    "            {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "        ],\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "optimizer = configure_optimizers(model, 1e-4, 1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-5)\n",
    "from torch.amp import autocast, GradScaler\n",
    "use_amp = False\n",
    "scaler = GradScaler(\"cuda\",enabled=use_amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e08983",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tensor_heatmap(criterion_bond_coh.compat_matrix.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_norm(loss, params_to_check,name):\n",
    "    if loss.item() == 0:\n",
    "        print(f\"{name} loss is zero, skipping gradient calculation.\")\n",
    "        return 0.0\n",
    "    fape_grads = torch.autograd.grad(loss, params_to_check, retain_graph=True, allow_unused=True)\n",
    "    total_norm = 0.0\n",
    "    for grad in fape_grads:\n",
    "        if grad is not None:\n",
    "            # 确保梯度不是空的 (如果allow_unused=True)\n",
    "            total_norm += grad.data.norm(2).item() ** 2\n",
    "    print( f\"{name} grad:\", total_norm ** 0.5)\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "\n",
    "with torch.autograd.detect_anomaly():\n",
    "    # for i in range(1000):\n",
    "    import time \n",
    "    total_time = 0\n",
    "\n",
    "\n",
    "    print(\"start \"*10)\n",
    "    for i, batch_data in enumerate(dataloader_train):\n",
    "        print(\"start \"*10)\n",
    "        # if (i+1) // 64 != 115 or(i+1) // 64 != 116:\n",
    "        #     continue\n",
    "        #with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=use_amp):\n",
    "\n",
    "        print(batch_data['pdb_id'])\n",
    "        start_time = time.time()\n",
    "        loss_frame, loss_seq, loss_bond, loss_clash, loss_FAPE, loss_torsion, loss_bond_coh,partial_T,  seq_pred, res_mask = model_forward(\n",
    "                                                        sampler,\n",
    "                                                        batch_data,\n",
    "                                                        criterion_frame,\n",
    "                                                        criterion_seq,\n",
    "                                                        criterion_bond,\n",
    "                                                        criterion_FAPE,\n",
    "                                                        criterion_torsion,\n",
    "                                                        criterion_bond_coh,\n",
    "                                                        criterion_clash=criterion_clash\n",
    "                                                        )\n",
    "\n",
    "        print(\"loss frame\",loss_frame.item())\n",
    "        print(\"loss seq\",loss_seq.item())\n",
    "        print(\"loss bond\",loss_bond.item())\n",
    "        print(\"loss clash\",loss_clash.item())\n",
    "        print(\"loss FAPE\",loss_FAPE.item())\n",
    "        print(\"loss torsion\",loss_torsion.item())\n",
    "        print(\"loss bond_coh\",loss_bond_coh.item())\n",
    "        print(\"partial_T\",partial_T)\n",
    "        pdb_id = batch_data['pdb_id']\n",
    "        #loss_total = loss_frame + loss_seq + loss_bond + loss_clash + loss_FAPE + loss_torsion\n",
    "\n",
    "            # 为了效率，我们只检查最后一个共享层的参数\n",
    "        # params_to_check = list(model.parameters())\n",
    "        \n",
    "        # loss_dict = {'frame':loss_frame, 'seq':loss_seq, 'bond':loss_bond, 'clash':loss_clash, 'FAPE':loss_FAPE, 'torsion':loss_torsion}\n",
    "        # if params_to_check: # 确保参数列表不为空\n",
    "        #     for name, loss in loss_dict.items():\n",
    "        #         print(f\"{name} loss:\", loss.item())\n",
    "        #         fape_grad_norm = calculate_norm(loss, params_to_check,name)\n",
    "\n",
    "        loss_total =  0.75*loss_frame+ loss_seq +  loss_bond + 0.2*loss_clash +   0.25*loss_FAPE  + 0.5* loss_torsion + loss_bond_coh\n",
    "        loss_total.backward()\n",
    "\n",
    "        # for name, p in sampler.model.named_parameters():\n",
    "        #     if p.requires_grad and p.grad is None:\n",
    "        #         print(\"No grad:\", name)\n",
    "\n",
    "        # is_grad_nan_or_inf = torch.tensor([False])\n",
    "        # for param in sampler.model.parameters():\n",
    "        #     if param.grad is not None and (torch.isinf(param.grad).any() or torch.isnan(param.grad).any()):\n",
    "        #         is_grad_nan_or_inf[0] = True\n",
    "        #         print(f\"Gradient is NaN or Inf for parameter: {param} at step {i} at pdb_id {pdb_id}\")\n",
    "        #         break # 发现一个就足够了，直接跳出循环\n",
    "        #scaler.scale(loss_total).backward()\n",
    "\n",
    "\n",
    "        # nn.utils.clip_grad_norm_(sampler.model.parameters(), 1)\n",
    "\n",
    "        # total_norm = 0\n",
    "        # for name, p in model.named_parameters():\n",
    "        #     if p.grad is not None:\n",
    "        #         param_norm = p.grad.data.norm(2)\n",
    "        #         #if param_norm.item()>1:\n",
    "        #         total_norm += param_norm.item() ** 2\n",
    "        # total_norm = total_norm ** 0.5\n",
    "        # print(f\"Total gradient norm: {total_norm}\")\n",
    "\n",
    "        # max_norm = 5\n",
    "        # torch.nn.utils.clip_grad_norm_(sampler.model.parameters(), max_norm=max_norm)\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        # if loss_bond.item() < 0.1:\n",
    "        #     print(\"loss_bond is too small, break\")\n",
    "        #     print(i)\n",
    "        #     break\n",
    "        # if loss_bond_coh>10:\n",
    "\n",
    "        print(\"all time:\", time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77416812",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader_train):\n",
    "    batch_data = batch\n",
    "    break\n",
    "#with torch.autograd.detect_anomaly():\n",
    "loss_frame, loss_seq, loss_bond, loss_clash, loss_FAPE, loss_torsion, loss_bond_coh,partial_T,  seq_pred, res_mask = model_forward(sampler,batch_data,\n",
    "                                                criterion_frame,\n",
    "                                                criterion_seq,\n",
    "                                                criterion_bond,\n",
    "                                                criterion_FAPE,\n",
    "                                                criterion_torsion,\n",
    "                                                criterion_bond_coh,\n",
    "                                                criterion_clash=criterion_clash\n",
    "                                                )\n",
    "print(\"loss frame\",loss_frame)\n",
    "print(\"loss seq\",loss_seq)\n",
    "print(\"loss bond\",loss_bond)\n",
    "print(\"loss clash\",loss_clash)\n",
    "print(\"loss FAPE\",loss_FAPE)\n",
    "print(\"loss torsion\",loss_torsion)\n",
    "print(\"loss bond_coh\",loss_bond_coh)\n",
    "print(\"partial_T\",partial_T)\n",
    "#loss_total = loss_frame + loss_seq + loss_bond + loss_clash + loss_FAPE + loss_torsion\n",
    "loss_total =  loss_bond_coh #loss_frame+ loss_seq + loss_bond + loss_clash +  loss_torsion +loss_FAPE  + loss_bond_coh\n",
    "loss_total.backward()\n",
    "\n",
    "\n",
    "# 监控并打印梯度范数\n",
    "total_norm = 0\n",
    "for name, p in model.named_parameters():\n",
    "    if p.grad is not None:\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        #if param_norm.item()>1:\n",
    "        print(name,param_norm.item())\n",
    "        total_norm += param_norm.item() ** 2\n",
    "total_norm = total_norm ** 0.5\n",
    "print(f\"Total gradient norm: {total_norm}\")\n",
    "\n",
    "#更新梯度\n",
    "\n",
    "\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
